{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark SQL Query Example\n",
    "\n",
    "The following Spark Job uses PySpark SQL engine to query Reporters from a Parquet file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from os.path import expanduser\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* Reading file ..: wsprspots-2020-11.snappy\n",
      "* Timestamp .....: 2020-12-31 05:50:15.525111\n",
      "* File Size .....: 1,151,378,369 bytes compressed\n",
      "* Read Time .....: 1.485 sec\n",
      "* Record Count ..: 77,337,023\n",
      "* Count Time ....: 0.899 sec\n",
      "\n",
      "Group, Count, Order by using DataFrame\n",
      "+--------+-------+\n",
      "|Reporter|  count|\n",
      "+--------+-------+\n",
      "|  EA8BFK|1120739|\n",
      "|  OE9GHV|1103335|\n",
      "|   WA2TP| 847124|\n",
      "|   KD2OM| 834896|\n",
      "|  IW2NKE| 818315|\n",
      "+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Query Time ....: 5.966 sec\n",
      "\n",
      "\n",
      "Group, Count, Order by using PySpark SQL Query\n",
      "+--------+--------+\n",
      "|Reporter|count(1)|\n",
      "+--------+--------+\n",
      "|  EA8BFK| 1120739|\n",
      "|  OE9GHV| 1103335|\n",
      "|   WA2TP|  847124|\n",
      "|   KD2OM|  834896|\n",
      "|  IW2NKE|  818315|\n",
      "+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Query Time ....: 5.368 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simple function to get file size\n",
    "def getSize(filename):\n",
    "    st = os.stat(filename)\n",
    "    return st.st_size\n",
    "\n",
    "# In-File Location ( normally an Azure Data Lake or AWS s3://<file-location-bucket> )\n",
    "home_dir = expanduser(\"~\")\n",
    "parquet_file = os.path.join(home_dir, 'Dev/Data/wspr/wsprspots-2020-11.snappy')\n",
    "file_size = getSize(parquet_file)\n",
    "\n",
    "# Setup the Spark Cluster Config Variables\n",
    "conf = SparkConf().setAppName(\"Radio Data Science Parquet Read\").setMaster(\"local[*]\")\n",
    "\n",
    "# Instantiate the Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Radio Data Science - Parquet Read Example\") \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Print some basic header information\n",
    "print(f'\\n* Reading file ..: {os.path.basename(parquet_file)}')\n",
    "print(f'* Timestamp .....: {datetime.datetime.now()}')\n",
    "print(f'* File Size .....: {file_size:,} bytes compressed')\n",
    "\n",
    "# Read the Parquet file\n",
    "start = time.time()\n",
    "df = spark.read.load(parquet_file, format=\"parquet\")\n",
    "end = time.time()\n",
    "print(f\"* Read Time .....: {round((end-start), 3)} sec\")\n",
    "\n",
    "# This is a second trip through the file to get a total count\n",
    "start = time.time()\n",
    "print(f'* Record Count ..: {df.count():,}')\n",
    "end = time.time()\n",
    "print(f\"* Count Time ....: {round((end-start), 3)} sec\")\n",
    "\n",
    "#\n",
    "# Example-1: Group by aggregation using DataFrames\n",
    "#\n",
    "print(f'\\nGroup, Count, Order by using DataFrame')\n",
    "start = time.time()\n",
    "df2 = df.groupBy('Reporter').count().orderBy('count', ascending=False)\n",
    "df2.show(5)\n",
    "end = time.time()\n",
    "print(f'Query Time ....: {round((end-start), 3)} sec\\n')\n",
    "\n",
    "#\n",
    "# Example-2: Group by aggregation using PySparkSQL ( sql query language )\n",
    "#\n",
    "print(f'\\nGroup, Count, Order by using PySpark SQL Query')\n",
    "start = time.time()\n",
    "df.createOrReplaceTempView(\"spot_data\")\n",
    "groupDF = spark.sql(\"SELECT Reporter, COUNT(*) FROM spot_data GROUP BY Reporter ORDER BY count(1) DESC\")\n",
    "groupDF.show(5)\n",
    "end = time.time()\n",
    "print(f\"Query Time ....: {round((end-start), 3)} sec\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown the PySpark engine.\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
