{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge CSV Files Demo\n",
    "\n",
    "The following PySpark job will merge all CSV files, de-deuplicate the results, followed by writing the merged file to a new location.\n",
    "\n",
    ">NOTE: All of the the data in the example was generated with Python Fake and Pandas. None of it is\n",
    ">real people / data\n",
    "\n",
    "## Script Actions\n",
    "\n",
    "The script below performs the following tasks\n",
    "\n",
    "* Reads all *.csv files in a folder locaiton\n",
    "* Merges' all files, then de-duplicates the final DataFrame\n",
    "* Writes the consolidated CSV to a new location\n",
    "\n",
    "```shell\n",
    "# Sample Folder Locations\n",
    "Input Dir   : Dev/Data/demo/{file0.csv,file1.csv,file2.csv}\n",
    "Output Dir  : Dev/Data/demo/merged/master.csv\n",
    "```\n",
    "\n",
    ">NOTE: Rather than listing files individually, you could use wildcards to pull\n",
    ">all files from a directory `/path-to-csv-files/*.csv`\n",
    "\n",
    "\n",
    "## CSV File Contents\n",
    "\n",
    "Three files were used to test the merge containing First, Last, and Birtdata Birthdate columns.\n",
    "\n",
    "* file0.csv has 10 unique entries\n",
    "* file1.csv has 10 unique entries plue (5) lines in file0.csv\n",
    "* file2.csv has 10 unique entries plue (10) lines in file1.csv\n",
    "\n",
    "The funciton below should combine the CSV files, and produce a master.csv without duplicates.\n",
    "\n",
    "\n",
    "## File Schema\n",
    "\n",
    "If the output format were `Parquet`, a schema should be used so it accompany's the partitions. As the output is\n",
    "CSV, no schema information is required as CSV files to not save schema data. However, using a schema can aid in changing columns names from then original CSV or add them is the original CSV does not contain a header-row.\n",
    "\n",
    ">NOTE: Python is `Not` Type-Safe. When using Python as a main data-processors, you should add\n",
    ">schemas where possible. Java and Scala are Type-Safe languages but it is still advised to use schemas\n",
    ">when you can.\n",
    "\n",
    "```python\n",
    "# Define schema ( Data-Types )\n",
    "csvSchema = StructType([ \\\n",
    "            StructField(\"First\", StringType(), True), \\\n",
    "            StructField(\"Last\", StringType(), True), \\\n",
    "            StructField(\"Birthdate\", StringType(), True)])\n",
    "```\n",
    "\n",
    "## Azure Blob Storage Access\n",
    "\n",
    "To use blob storage on Azure, its merely a matter of mounting the path in the Spark Job, then setting \n",
    "the input / output locations you desire.\n",
    "\n",
    "For a full examples, see : [Run a Databricks notebook with the Databricks Notebook Activity in Azure Data Factory](https://docs.microsoft.com/en-us/azure/data-factory/transform-data-using-databricks-notebook)\n",
    "\n",
    "The Azure link above demostrates how to:\n",
    "\n",
    "* Create a data factory.\n",
    "* Create a pipeline that uses Databricks Notebook Activity.\n",
    "* Trigger a pipeline run.\n",
    "* Monitor the pipeline run.\n",
    "\n",
    "```python\n",
    "# Mount the remote location\n",
    "dbutils.fs.mount(\n",
    "  source = \"wasbs://<container-name>@<storage-account-name>.blob.core.windows.net\",\n",
    "  mount_point = \"/mnt/<mount-name>\",\n",
    "  extra_configs = {\"<conf-key>\":dbutils.secrets.get(scope = \"<scope-name>\", key = \"<key-name>\")})\n",
    "\n",
    "# Set the path in your script\n",
    "df = spark.read.csv(\"/mnt/<mount-name>/...\")\n",
    "df = spark.read.csv(\"dbfs:/<mount-name>/...\")\n",
    "```\n",
    "\n",
    "## Databricks Workbook\n",
    "\n",
    "The following script is what would be saved to a [Databricks Notebook](https://docs.databricks.com/notebooks/index.html) then run directly, or via [Azure Datafactory Pipline](https://docs.microsoft.com/en-us/azure/data-factory/transform-data-using-databricks-notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import expanduser\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+\n",
      "|   First|      Last| Birthdate|\n",
      "+--------+----------+----------+\n",
      "| Anatole|    Brekke|1978-07-05|\n",
      "|   Arman|    Hudson|1959-06-23|\n",
      "|   Aubra|   Pollich|2001-03-13|\n",
      "|   Belen|    Klocko|1962-01-24|\n",
      "|   Bobby|     Hilll|1989-08-24|\n",
      "| Cherrie|    Nienow|1955-04-14|\n",
      "|Chrystal|    Zemlak|1999-01-04|\n",
      "|   Cliff|    Hamill|1966-01-30|\n",
      "| Danniel|   Shields|1961-10-19|\n",
      "| Delaney|     Morar|2007-10-21|\n",
      "|    Evia|   Waelchi|1953-04-13|\n",
      "|  Gladis|   Collier|1982-10-11|\n",
      "|  Gustav|     Bauch|1974-02-06|\n",
      "|  Harlie|     Borer|1965-03-30|\n",
      "|  Hobart|   Keebler|1968-09-25|\n",
      "|Humphrey|     Thiel|1949-05-04|\n",
      "|   Jenna|    Feeney|1966-02-16|\n",
      "|    Jere|     Pagac|1944-04-04|\n",
      "|  Justen|  Champlin|1994-10-02|\n",
      "|   Kenji|    Harvey|2007-03-27|\n",
      "|   Mario|Bartoletti|1981-03-07|\n",
      "|   Mario|     Nader|1993-05-02|\n",
      "|  Neppie|     Runte|1949-05-31|\n",
      "|    Olof|   McGlynn|1982-10-07|\n",
      "|   Ruthe|     Emard|1958-10-14|\n",
      "|   Selah|  Reynolds|1988-06-16|\n",
      "|  Tawnya|    Dooley|1951-12-26|\n",
      "|   Telly|     Kling|1952-02-16|\n",
      "|   Venie|     Boyle|1986-04-17|\n",
      "| Wendell|      Bode|1941-11-23|\n",
      "+--------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In-File Location ( Azure Data Lake or AWS s3://<file-location-bucket> )\n",
    "home_dir = expanduser(\"~\")\n",
    "inFiles = os.path.join(home_dir, 'Dev/Data/demo/*.csv')\n",
    "mergeDir = os.path.join(home_dir, 'Dev/Data/demo/merged/')\n",
    "\n",
    "# Define schema ( Data-Types )\n",
    "csvSchema = StructType([ \\\n",
    "            StructField(\"First\", StringType(), True), \\\n",
    "            StructField(\"Last\", StringType(), True), \\\n",
    "            StructField(\"Birthdate\", StringType(), True)])\n",
    "\n",
    "# Setup the Spark Cluster Config Variables\n",
    "conf = SparkConf().setAppName(\"Project-1 Merge CSV Files\").setMaster(\"local[*]\")\n",
    "\n",
    "# Instantiate the Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create the DataSet, Read and De-Duplicatate\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"True\") \\\n",
    "    .option(\"sep\",\",\") \\\n",
    "    .schema(csvSchema) \\\n",
    "    .load(inFiles) \\\n",
    "    .dropDuplicates()\n",
    "\n",
    "# Show the contents of merged files\n",
    "df.orderBy('First', 'Last', ascending=True).show(40)\n",
    "\n",
    "# Write the master CSV file\n",
    "df.toPandas().to_csv(os.path.join(mergeDir, 'project1.csv'), header=True, sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown the PySpark engine.\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
