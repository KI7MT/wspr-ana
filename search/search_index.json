{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"WSPR Analytics \u00b6 In the early days (March 2008), WSPR Spots measured in the hundreds of thousands per month. Today, that number has increased to over 75+ Million per month and shows no sign of abatement. By any reasonable definition, it is safe to say that WSPR has entered the realm of Big Data . Project Goal \u00b6 The goal of this project is to provide a set of tools to download, manage, transform and query WSPR DataSets using modern Big Data frameworks. Documentation \u00b6 Each folder will contain a seris of README files that explain the content, and where warrented, usage. Additionally, project documentation website can be used for more extensive and exhaustinve explination of the project and its contence. WSPR Analytics Docs - bookmark this location for future reference Folder Descriptions \u00b6 Several frameworks are used in this repository. The following matrix provides a short description of each, and their intended purpose. docs - Python, MkDocs for General repository documentation experiments - Python, Spark, PyArrow Scripts to manage CSV & Parquet files java - |Java, Maven, SBT Basic Java apps for RDD and Avro examples notebooks - Jupyter Notebooks Notebooks to demonstrate features an capabilities scala - Scala programs to query files and convert csv files wsprana - Python package to manage downloading CSV files (soon to be retired) Pay close attention to the README files as they lay out how to setup the tools needed to run their respective scripts or application. Basic Tool Requirements \u00b6 NOTE: During development, the wsprana package is not intended for pip installaiton yet, but will be.It should be checked out and run from source at present. You must have Python, Java, PySpark/Spark available from the command line. Java openjdk version 1.8.0_275 or later Python 3.7+ PySpark from PyPi Apache Arrow 2.0+ Scala 2.12+ Spark 3.0.1 PostgreSQL Database (local, remote, Docker, etc) An easy way (on Linux / MacOS) to manage Java, Spark, Scala and SBT is through an management tool called sdkman . This tool allows one to install virtually any combination of tools you need without affecting your root file system. With the exception of Python, All the above requirements can be installed and managed via sdkman . For Python, the recomendation is to use Anaconda Python , the full version, as it provides all the analytics tooling you'll need for this project and more. Data Sources and Processing \u00b6 The primary data source will be the monthly WSPRNet Archives . At present, there is no plan to pull nightly updates. That could change if a reasonble API is identified. The WSPR CSV tools will be used to convert the raw CSV files into a format better suited for parallel processing, namely, Parquet . Read speeds, storage footprints, and ingestion improve dramativaly with this storage format. However, there is a drawback, one cannot simply view a binary file as they can with raw text files. The original CSV will remain in place, but all bulk processing will be pulled from Parquet . During these transformations is where PyArrow + PySpark will earn it's keep. Persistant Storage \u00b6 A PostgreSQL database server will be needed. There are many ways to perform this installation (local, remote, Dockerize PostgreSQL , PostgreSQL with Vagrant , etc). Whichever method you chose, it will be used extensively by many of the apps and scripts. WSPR Analytics is Apache 2.0 licensed code.","title":"Home"},{"location":"#wspr-analytics","text":"In the early days (March 2008), WSPR Spots measured in the hundreds of thousands per month. Today, that number has increased to over 75+ Million per month and shows no sign of abatement. By any reasonable definition, it is safe to say that WSPR has entered the realm of Big Data .","title":"WSPR Analytics"},{"location":"#project-goal","text":"The goal of this project is to provide a set of tools to download, manage, transform and query WSPR DataSets using modern Big Data frameworks.","title":"Project Goal"},{"location":"#documentation","text":"Each folder will contain a seris of README files that explain the content, and where warrented, usage. Additionally, project documentation website can be used for more extensive and exhaustinve explination of the project and its contence. WSPR Analytics Docs - bookmark this location for future reference","title":"Documentation"},{"location":"#folder-descriptions","text":"Several frameworks are used in this repository. The following matrix provides a short description of each, and their intended purpose. docs - Python, MkDocs for General repository documentation experiments - Python, Spark, PyArrow Scripts to manage CSV & Parquet files java - |Java, Maven, SBT Basic Java apps for RDD and Avro examples notebooks - Jupyter Notebooks Notebooks to demonstrate features an capabilities scala - Scala programs to query files and convert csv files wsprana - Python package to manage downloading CSV files (soon to be retired) Pay close attention to the README files as they lay out how to setup the tools needed to run their respective scripts or application.","title":"Folder Descriptions"},{"location":"#basic-tool-requirements","text":"NOTE: During development, the wsprana package is not intended for pip installaiton yet, but will be.It should be checked out and run from source at present. You must have Python, Java, PySpark/Spark available from the command line. Java openjdk version 1.8.0_275 or later Python 3.7+ PySpark from PyPi Apache Arrow 2.0+ Scala 2.12+ Spark 3.0.1 PostgreSQL Database (local, remote, Docker, etc) An easy way (on Linux / MacOS) to manage Java, Spark, Scala and SBT is through an management tool called sdkman . This tool allows one to install virtually any combination of tools you need without affecting your root file system. With the exception of Python, All the above requirements can be installed and managed via sdkman . For Python, the recomendation is to use Anaconda Python , the full version, as it provides all the analytics tooling you'll need for this project and more.","title":"Basic Tool Requirements"},{"location":"#data-sources-and-processing","text":"The primary data source will be the monthly WSPRNet Archives . At present, there is no plan to pull nightly updates. That could change if a reasonble API is identified. The WSPR CSV tools will be used to convert the raw CSV files into a format better suited for parallel processing, namely, Parquet . Read speeds, storage footprints, and ingestion improve dramativaly with this storage format. However, there is a drawback, one cannot simply view a binary file as they can with raw text files. The original CSV will remain in place, but all bulk processing will be pulled from Parquet . During these transformations is where PyArrow + PySpark will earn it's keep.","title":"Data Sources and Processing"},{"location":"#persistant-storage","text":"A PostgreSQL database server will be needed. There are many ways to perform this installation (local, remote, Dockerize PostgreSQL , PostgreSQL with Vagrant , etc). Whichever method you chose, it will be used extensively by many of the apps and scripts. WSPR Analytics is Apache 2.0 licensed code.","title":"Persistant Storage"},{"location":"examples/overview/","text":"Under Development","title":"Overview"},{"location":"notebooks/overview/","text":"Under Development","title":"Overview"},{"location":"pyspark/overview/","text":"Apache Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for incremental computation and stream processing. -- Apache Spark Project Apache Spark is written in a programming language called Scala , which runs on a Java Virtual Machine (JVM). To allow for an easy integration between Scala and Python, the programming community created a tool set (libraries/modules/scripts) called PySpark . Other languages such as Ruby, R, and Java also have bindings that support or work with the Spark framework. It can get a bit confusing trying to distinguish the manay moving parts of the Spark echosystem. To make it a bit easier, just remember Spark is the foundation, and PySpark in merely an integration to that foundation. For the purposes of the WSPR Analytics project, we'll being using PySpark as a easy gateway to Jupyter Notebooks in a distributive manner. As with Spark , PySpark has it's own shell when not being forwarded to Jupyter Notebooks . However, both still rely on Spark as the compute engine, and in this case, Python is the interface via the command-line rather than web page. You can see by the shell below, I was running Anaconda Python v3.7.9 from with Spark v3.0.1. You can also see that the pyspark shell creates a SparkContext for us upon entry. This will be better understood when executng example scripts. Python 3.7.9 (default, Aug 31 2020, 12:42:55) [GCC 7.3.0] :: Anaconda, Inc. on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 3.0.1 /_/ Using Python version 3.7.9 (default, Aug 31 2020 12:42:55) SparkSession available as 'spark'. >>> spark.version '3.0.1' >>> sc.version '3.0.1' Additional Resources \u00b6 There are many high-quailty sites that cover all aspects of PySpark in great detail: Databricks - The original creator of Spark Learning Spark 2 nd Edition , also from Databricks, is a great book PySpark Docs covers SQL, ML, Streaming and more Spark itself from the Apache Foundation PySpark at JavaPoint is a great resource for all things coding","title":"Overview"},{"location":"pyspark/overview/#additional-resources","text":"There are many high-quailty sites that cover all aspects of PySpark in great detail: Databricks - The original creator of Spark Learning Spark 2 nd Edition , also from Databricks, is a great book PySpark Docs covers SQL, ML, Streaming and more Spark itself from the Apache Foundation PySpark at JavaPoint is a great resource for all things coding","title":"Additional Resources"},{"location":"rscripts/installation/","text":"Installation \u00b6 At the time of this writing, usage has only been tested on Linux x86-64 and. There is no particular reason why installation and usage could not be done on Windows or Mac OSX if the Requirements are met. It is Highly Recommended to use Anaconda from Continuum Analytics. It's free, supports all major operating systems, is well supported, does not interfere with system level packaging, can be installed on a per user basics and provides everything needed for Comprehensive WSPR Data Analysis . Installing Anaconda \u00b6 Installing Anaconda is very easy. Simply download the shell script and run it in a terminal: Use their Install Instructions paying particular atention to the additional package needs for Qt. Follow the prompts and when asked, I elected to add the the source scripts to my .bashrc but that is entirely up to you. I also used the default installation direcotry of: /home/$USER/anaconda3 Upgrade Anaconda \u00b6 This is part of 30 Minute Conda getting started page, but for completeness, I'm adding what was needed for my environment. All actions are performed in a terminal, open as required on your system, then: First, Anaconda should be upgraded: conda update conda Next, update the conda-env scripts: conda update conda-env That is all for the basic Anaconda installation and update. You should close, then re-open your terminal to ensure all the paths and updates are working proerpy. Additional Python Modules \u00b6 One package that is not available is a conda package is AppDirs but can be installed with Pip . In the terinal: pip install appdirs clint Installing R \u00b6 The R-Scripting language in not part of the base Anaconda installation, however, installation is fairly easy using conda , the Anaconda package manager. Again, in the terminal, perform the following: conda install -c r r-essentials conda install -c r r-gridextra","title":"Installation"},{"location":"rscripts/installation/#installation","text":"At the time of this writing, usage has only been tested on Linux x86-64 and. There is no particular reason why installation and usage could not be done on Windows or Mac OSX if the Requirements are met. It is Highly Recommended to use Anaconda from Continuum Analytics. It's free, supports all major operating systems, is well supported, does not interfere with system level packaging, can be installed on a per user basics and provides everything needed for Comprehensive WSPR Data Analysis .","title":"Installation"},{"location":"rscripts/installation/#installing-anaconda","text":"Installing Anaconda is very easy. Simply download the shell script and run it in a terminal: Use their Install Instructions paying particular atention to the additional package needs for Qt. Follow the prompts and when asked, I elected to add the the source scripts to my .bashrc but that is entirely up to you. I also used the default installation direcotry of: /home/$USER/anaconda3","title":"Installing Anaconda"},{"location":"rscripts/installation/#upgrade-anaconda","text":"This is part of 30 Minute Conda getting started page, but for completeness, I'm adding what was needed for my environment. All actions are performed in a terminal, open as required on your system, then: First, Anaconda should be upgraded: conda update conda Next, update the conda-env scripts: conda update conda-env That is all for the basic Anaconda installation and update. You should close, then re-open your terminal to ensure all the paths and updates are working proerpy.","title":"Upgrade Anaconda"},{"location":"rscripts/installation/#additional-python-modules","text":"One package that is not available is a conda package is AppDirs but can be installed with Pip . In the terinal: pip install appdirs clint","title":"Additional Python Modules"},{"location":"rscripts/installation/#installing-r","text":"The R-Scripting language in not part of the base Anaconda installation, however, installation is fairly easy using conda , the Anaconda package manager. Again, in the terminal, perform the following: conda install -c r r-essentials conda install -c r r-gridextra","title":"Installing R"},{"location":"scala/overview/","text":"Under development","title":"Overview"},{"location":"setup/install-java/","text":"Under Development","title":"Installing Java"},{"location":"setup/install-python/","text":"Under Development","title":"Installing Python"},{"location":"setup/install-scala/","text":"Under Development","title":"Installing Scala"},{"location":"setup/install-sdkman/","text":"Under Development","title":"Installing Sdkman"},{"location":"setup/overview/","text":"Under Development","title":"Overview"}]}