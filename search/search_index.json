{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"WSPR Analytics \u00b6 In the early days (March 2008), WSPR Spots measured in the hundreds of thousands per month. Today, that number has increased to over 46 Million per month at last count, and shows no sign of abatement. By any reasonable definition, it is safe to say, WSPR has entered the realm of Big Data . WSPR Analytics CSV Utilities is one of several packages surrouning the WSPR Analytics Suite of tools. This repository contains tools used to download, clean, transform and otherwise manage the original CSV files from WSPRnet . Other tools will be used to generate additional datasets for analytics purposes. Folder Descriptions \u00b6 Several frameworks are used in this repository as there is no central application. The following is a shor description of each, and their intended purpose. docs - Python, MkDocs for General repository documentation examples - Python, Spark, PyArrow to manage CSV & Parquet files java - Java, Maven, SBT for basic RDD and Avro examples notebooks - Jupyter Notebooks to demonstrate features an capabilities scala - Scala programs to query DataSets wsprana - Python package to manage downloading of CSV files Each folder has a top level README, as do each sub-folder project. One should pay close attention to these files as they lay out how to setup the tools needed to run their respective scripts. Basic Tool Requirements \u00b6 NOTE: During development, the wsprana package is not intended for pip installaiton yet, but will be.It should be checked out and run from source at present. You must have Python, Java, PySpark/Spark available from the command line. Java openjdk version 1.8.0_275 or later Python 3.7+ PySpark from PyPi Apache Arrow 2.0+ Scala 2.12+ Spark 3.0.1 PostgreSQL Database (local, remote, Docker, etc) Optional \u00b6 If you plan to run on a true cluster, or plan on partitioning your data, installing one or more of the following could be useful. Hadoop Cassandra Hive Pig K8S Docker Submarine For a full list of tooling, see the Apache Foundation Project List . An easy way (on Linux / MacOS) to mange Java, Spark, Scala and SBT is through an management tool called sdkman . This tool allows one to install virtually any combination of tools you need without affecting your root file system. With the exception of Python, All the above requirements can be installed and managed via sdkman . For Python, the recomendation is to use Anaconda Python , the full version, as it provides all the analytics tooling you'll need for this project and more. Data Sources and Processing \u00b6 The primary data source will be the monthly WSPRNet Archives . At present, there is no plan to pull nightly updates. That could change if a reasonble API is identified. The WSPR CSV tools will be used to convert the raw CSV files into a format better suited for parallel processing, namely, Parquet . Read speeds, storage footprints, and ingestion improve dramativaly with this storage format. However, there is a drawback, one cannot simply view a binary file as they can with raw text files. The original CSV will remain in place, but all bulk processing will be pulled from Parquet . During these transformations is where PyArrow + PySpark will earn it's keep. A PostgreSQL database server will be needed. There are many ways to perform this installation (local, remote, Dockerize PostgreSQL , PostgreSQL with Vagrant , etc). Whichever method you chose, it will be used extensively by many of the apps and scripts. WSPR Analytics is Apache 2.0 licensed code.","title":"Home"},{"location":"#wspr-analytics","text":"In the early days (March 2008), WSPR Spots measured in the hundreds of thousands per month. Today, that number has increased to over 46 Million per month at last count, and shows no sign of abatement. By any reasonable definition, it is safe to say, WSPR has entered the realm of Big Data . WSPR Analytics CSV Utilities is one of several packages surrouning the WSPR Analytics Suite of tools. This repository contains tools used to download, clean, transform and otherwise manage the original CSV files from WSPRnet . Other tools will be used to generate additional datasets for analytics purposes.","title":"WSPR Analytics"},{"location":"#folder-descriptions","text":"Several frameworks are used in this repository as there is no central application. The following is a shor description of each, and their intended purpose. docs - Python, MkDocs for General repository documentation examples - Python, Spark, PyArrow to manage CSV & Parquet files java - Java, Maven, SBT for basic RDD and Avro examples notebooks - Jupyter Notebooks to demonstrate features an capabilities scala - Scala programs to query DataSets wsprana - Python package to manage downloading of CSV files Each folder has a top level README, as do each sub-folder project. One should pay close attention to these files as they lay out how to setup the tools needed to run their respective scripts.","title":"Folder Descriptions"},{"location":"#basic-tool-requirements","text":"NOTE: During development, the wsprana package is not intended for pip installaiton yet, but will be.It should be checked out and run from source at present. You must have Python, Java, PySpark/Spark available from the command line. Java openjdk version 1.8.0_275 or later Python 3.7+ PySpark from PyPi Apache Arrow 2.0+ Scala 2.12+ Spark 3.0.1 PostgreSQL Database (local, remote, Docker, etc)","title":"Basic Tool Requirements"},{"location":"#optional","text":"If you plan to run on a true cluster, or plan on partitioning your data, installing one or more of the following could be useful. Hadoop Cassandra Hive Pig K8S Docker Submarine For a full list of tooling, see the Apache Foundation Project List . An easy way (on Linux / MacOS) to mange Java, Spark, Scala and SBT is through an management tool called sdkman . This tool allows one to install virtually any combination of tools you need without affecting your root file system. With the exception of Python, All the above requirements can be installed and managed via sdkman . For Python, the recomendation is to use Anaconda Python , the full version, as it provides all the analytics tooling you'll need for this project and more.","title":"Optional"},{"location":"#data-sources-and-processing","text":"The primary data source will be the monthly WSPRNet Archives . At present, there is no plan to pull nightly updates. That could change if a reasonble API is identified. The WSPR CSV tools will be used to convert the raw CSV files into a format better suited for parallel processing, namely, Parquet . Read speeds, storage footprints, and ingestion improve dramativaly with this storage format. However, there is a drawback, one cannot simply view a binary file as they can with raw text files. The original CSV will remain in place, but all bulk processing will be pulled from Parquet . During these transformations is where PyArrow + PySpark will earn it's keep. A PostgreSQL database server will be needed. There are many ways to perform this installation (local, remote, Dockerize PostgreSQL , PostgreSQL with Vagrant , etc). Whichever method you chose, it will be used extensively by many of the apps and scripts. WSPR Analytics is Apache 2.0 licensed code.","title":"Data Sources and Processing"},{"location":"examples/overview/","text":"Under Development","title":"Overview"},{"location":"notebooks/overview/","text":"Under Development","title":"Overview"},{"location":"pyspark/overview/","text":"Apache Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for incremental computation and stream processing. -- Apache Spark Project Apache Spark is written in a programming language called Scala , which runs on a Java Virtual Machine (JVM). To allow for an easy integration between Scala and Python, the programming community created a tool set (libraries/modules/scripts) called PySpark . Other languages such as Ruby, R, and Java also have bindings that support or work with the Spark framework. It can get a bit confusing trying to distinguish the manay moving parts of the Spark echosystem. To make it a bit easier, just remember Spark is the foundation, and PySpark in merely an integration to that foundation. For the purposes of the WSPR Analytics project, we'll being using PySpark as a easy gateway to Jupyter Notebooks in a distributive manner. As with Spark , PySpark has it's own shell when not being forwarded to Jupyter Notebooks . However, both still rely on Spark as the compute engine, and in this case, Python is the interface via the command-line rather than web page. You can see by the shell below, I was running Anaconda Python v3.7.9 from with Spark v3.0.1. You can also see that the pyspark shell creates a SparkContext for us upon entry. This will be better understood when executng example scripts. Python 3.7.9 (default, Aug 31 2020, 12:42:55) [GCC 7.3.0] :: Anaconda, Inc. on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 3.0.1 /_/ Using Python version 3.7.9 (default, Aug 31 2020 12:42:55) SparkSession available as 'spark'. >>> spark.version '3.0.1' >>> sc.version '3.0.1' Additional Resources \u00b6 There are many high-quailty sites that cover all aspects of PySpark in great detail: Databricks - The original creator of Spark Learning Spark 2 nd Edition , also from Databricks, is a great book PySpark Docs covers SQL, ML, Streaming and more Spark itself from the Apache Foundation PySpark at JavaPoint is a great resource for all things coding","title":"Overview"},{"location":"pyspark/overview/#additional-resources","text":"There are many high-quailty sites that cover all aspects of PySpark in great detail: Databricks - The original creator of Spark Learning Spark 2 nd Edition , also from Databricks, is a great book PySpark Docs covers SQL, ML, Streaming and more Spark itself from the Apache Foundation PySpark at JavaPoint is a great resource for all things coding","title":"Additional Resources"},{"location":"rscripts/installation/","text":"Installation \u00b6 At the time of this writing, usage has only been tested on Linux x86-64 and. There is no particular reason why installation and usage could not be done on Windows or Mac OSX if the Requirements are met. It is Highly Recommended to use Anaconda from Continuum Analytics`. It's free, supports all major operating systems, is well supported, does not interfere with system level packaging, can be installed on a per user basics and provides everything needed for Comprehensive WSPR Data Analysis . Installing Anaconda \u00b6 Installing Anaconda is very easy. Simply download the shell script and run it in a terminal: bash Anaconda3-4.0.0-Linux-x86_64.sh Follow the prompts and when asked, I elected to add the the source scripts to my .bashrc but that is entirely up to you. I also used the default installation direcotry of: /home/$USER/anaconda3 Upgrade Anaconda \u00b6 This is part of 30 Minute Conda getting started page, but for completeness, I'm adding what was needed for my environment. All actions are performed in a terminal, open as required on your system, then: First, Anaconda should be upgraded: conda update conda Next, update the conda-env scripts: conda update conda-env That is all for the basic Anaconda installation and update. You should close, then re-open your terminal to ensure all the paths and updates are working proerpy. Additional Python Modules \u00b6 One package that is not available is a conda package is AppDirs but can be installed with Pip . In the terinal: pip install appdirs clint Installing R \u00b6 The R-Scripting language in not part of the base Anaconda installation, however, installation is fairly easy using conda , the Anaconda package manager. Again, in the terminal, perform the following: conda install -c r r-essentials conda install -c r r-gridextra","title":"Installation"},{"location":"rscripts/installation/#installation","text":"At the time of this writing, usage has only been tested on Linux x86-64 and. There is no particular reason why installation and usage could not be done on Windows or Mac OSX if the Requirements are met. It is Highly Recommended to use Anaconda from Continuum Analytics`. It's free, supports all major operating systems, is well supported, does not interfere with system level packaging, can be installed on a per user basics and provides everything needed for Comprehensive WSPR Data Analysis .","title":"Installation"},{"location":"rscripts/installation/#installing-anaconda","text":"Installing Anaconda is very easy. Simply download the shell script and run it in a terminal: bash Anaconda3-4.0.0-Linux-x86_64.sh Follow the prompts and when asked, I elected to add the the source scripts to my .bashrc but that is entirely up to you. I also used the default installation direcotry of: /home/$USER/anaconda3","title":"Installing Anaconda"},{"location":"rscripts/installation/#upgrade-anaconda","text":"This is part of 30 Minute Conda getting started page, but for completeness, I'm adding what was needed for my environment. All actions are performed in a terminal, open as required on your system, then: First, Anaconda should be upgraded: conda update conda Next, update the conda-env scripts: conda update conda-env That is all for the basic Anaconda installation and update. You should close, then re-open your terminal to ensure all the paths and updates are working proerpy.","title":"Upgrade Anaconda"},{"location":"rscripts/installation/#additional-python-modules","text":"One package that is not available is a conda package is AppDirs but can be installed with Pip . In the terinal: pip install appdirs clint","title":"Additional Python Modules"},{"location":"rscripts/installation/#installing-r","text":"The R-Scripting language in not part of the base Anaconda installation, however, installation is fairly easy using conda , the Anaconda package manager. Again, in the terminal, perform the following: conda install -c r r-essentials conda install -c r r-gridextra","title":"Installing R"},{"location":"scala/overview/","text":"Under development","title":"Overview"},{"location":"setup/install-java/","text":"Under Development","title":"Installing Java"},{"location":"setup/install-python/","text":"Under Development","title":"Installing Python"},{"location":"setup/install-scala/","text":"Under Development","title":"Installing Scala"},{"location":"setup/install-sdkman/","text":"Under Development","title":"Installing Sdkman"},{"location":"setup/overview/","text":"Under Development","title":"Overview"}]}