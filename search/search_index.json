{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"WSPR Analytics \u00b6 In the early days (March 2008), WSPR Spots measured in the hundreds of thousands per month. Today, that number has increased to over 75+ Million per month and shows no sign of abatement. By any reasonable definition, it is safe to say that WSPR has entered the realm of Big Data . Project Goal \u00b6 The goal of this project is to provide a set of tools to download, manage, transform and query WSPR DataSets using modern Big Data frameworks. Folder Descriptions \u00b6 Several frameworks are used in this repository. The following matrix provides a short description of each, and their intended purpose. docs - Python, MkDocs for repository documentation java - Java, Maven, and SBT apps for RDD and Avro examples notebooks - Jupyter Notebooks for basic test and visualization pyspark - Python, PyArrow scripts that interact with CSV and Parquet files spark -Scala programs to perform tasks wsprdaemon - Python, Scala, Psql utilities related to the WSPR Daemon project wsprana - Python, (soon to be retired) Pay close attention to the README files as they lay out how to setup the tools needed to run their respective scripts or application. Basic Tool Requirements \u00b6 NOTE: During development, the wsprana package is not intended for pip installaiton yet, but will be.It should be checked out and run from source at present. You must have Python, Java, PySpark/Spark available from the command line. Java openjdk version 1.8.0_275 or later Python 3.7+ PySpark from PyPi Apache Arrow 2.0+ Scala 2.12+ Spark 3.0.1 PostgreSQL Database (local, remote, Docker, etc) An easy way (on Linux / MacOS) to manage Java, Spark, Scala and SBT is through an management tool called sdkman . This tool allows one to install virtually any combination of tools you need without affecting your root file system. With the exception of Python, All the above requirements can be installed and managed via sdkman . For Python, the recomendation is to use Anaconda Python , the full version, as it provides all the analytics tooling you'll need for this project and more. Data Sources and Processing \u00b6 The primary data source will be the monthly WSPRNet Archives . At present, there is no plan to pull nightly updates. That could change if a reasonble API is identified. The WSPR CSV tools will be used to convert the raw CSV files into a format better suited for parallel processing, namely, Parquet . Read speeds, storage footprints, and ingestion improve dramativaly with this storage format. However, there is a drawback, one cannot simply view a binary file as they can with raw text files. The original CSV will remain in place, but all bulk processing will be pulled from Parquet . During these transformations is where PyArrow + PySpark will earn it's keep. Persistant Storage \u00b6 A PostgreSQL database server will be needed. There are many ways to perform this installation (local, remote, Dockerize PostgreSQL , PostgreSQL with Vagrant , etc). Whichever method you chose, it will be used extensively by many of the apps and scripts. Distribution Tabs \u00b6 In many of the installation sections, you will see Tabs for a particular distribution. Clicking on the desired tab will render the command or content relevant to that distribution. NOTE: These are just examples, and not intended for actual use. Alpine Update the package list apk update Add a package apk add openssh apk add openssh opentp vim Ubuntu Upgrade the host System Packages. # Run the following command sudo apt-get update && sudo apt-get upgrade Mint Install a pre-requesite package for VirtualBox. # Run the following command sudo apt-get update sudo apt-get install dkms Fedora a. Update your fedora release sudo dnf upgrade --refresh b. Install a plugin sudo dnf install dnf-plugin-system-upgrade c. Download upgraded packages sudo dnf system-upgrade download --refresh --releasever = 33 Windows Lets not and say we did! REM Run the following command echo Spark runs better on Linux. echo Please consider running Spark apps in echo VirtualBox if your host os is Windows!! Super Fencing \u00b6 In many examples you may see multiple tabs relating to a particular code-block. Clicking on each tab shows the syntax for the stated language. This is the same behaviour as with Distribution Tabs C #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } C++ #include <iostream> int main ( void ) { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; } Scala /** * * Convert Epoch in from WSPRnet CSV files * */ def main ( args : Array [ String ]) : Unit = { val debug : Boolean = false // make Java's log4j warnings be quiet PropertyConfigurator . configure ( \"log4j/log4j.properties\" ) // IMPORTANT: When converting EPOCH times, you must do so with the // to_utc_timestamp method. This requires telling the system what Zone // your computer is in (the one doing the conversion) in order to get // the correct unix time. val z = ZoneId . systemDefault () val zoneId = z . getId println ( \"Process Steps For Processing A CSV File\" ) println ( \"- Create a Spark Session\" ) // Create the SPark Session val spark : SparkSession = SparkSession . builder () . appName ( \"Read CSV and Show Schema\" ) . master ( \"local[16]\" ) . getOrCreate () // Add Type-Safe Schema println ( \"- Create the Spot Schema\" ) val spotSchema = new StructType () . add ( \"SpotID\" , LongType , nullable = false ) . add ( \"Timestamp\" , IntegerType , nullable = false ) . add ( \"Reporter\" , StringType , nullable = false ) . add ( \"RxGrid\" , StringType , nullable = false ) . add ( \"SNR\" , ByteType , nullable = false ) . add ( \"Frequency\" , DoubleType , nullable = false ) . add ( \"CallSign\" , StringType , nullable = false ) . add ( \"Grid\" , StringType , nullable = false ) . add ( \"Power\" , ByteType , nullable = false ) . add ( \"Drift\" , ByteType , nullable = false ) . add ( \"Distance\" , ShortType , nullable = false ) . add ( \"Azimuth\" , ByteType , nullable = false ) . add ( \"Band\" , ByteType , nullable = false ) . add ( \"Version\" , StringType , nullable = true ) . add ( \"Code\" , ByteType , nullable = true ) // Create the Spark DataSet ( using small 100K csv ) println ( \"- Read the CSV file into a DataSet\" ) import spark.implicits._ val ds = spark . read . option ( \"delimiter\" , \",\" ) . option ( \"header\" , \"false\" ) . schema ( spotSchema ) . csv ( path = \"data/spots-2020-02-100K.csv\" ) . as [ RawSpot ] println ( \"- Select the column we want to process\" ) // Filter the data set val res = ds . select ( \"*\" ) . withColumn ( \"x_TimeStamp\" , date_format ( col ( \"TimeStamp\" ) . cast ( DataTypes . TimestampType ), \"yyyy-MM-dd HH:mm:ss\" )) // only print the schema in Debug Mode if ( debug ) { res . printSchema () } // See note above about ZoneId, it's important! println ( \"- Setup Epoh Conversion\" ) val res1 = res . select ( \"*\" ) . withColumn ( \"x_timestamp\" , to_utc_timestamp ( col ( \"x_TimeStamp\" ), zoneId )) . withColumn ( \"x_date\" , to_date ( col ( \"x_TimeStamp\" ))) . withColumn ( \"x_year\" , year ( col ( \"x_TimeStamp\" )). cast ( ShortType )) . withColumn ( \"x_month\" , month ( col ( \"x_TimeStamp\" )). cast ( ByteType )) . withColumn ( \"x_day\" , dayofmonth ( col ( \"x_TimeStamp\" )). cast ( ByteType )) . withColumn ( \"x_hour\" , hour ( col ( \"x_TimeStamp\" )). cast ( ByteType )) . withColumn ( \"x_minute\" , minute ( col ( \"x_TimeStamp\" )). cast ( ByteType )) // only print the schema in Debug Mode if ( debug ) { println ( \"- Print Res1 Schema\" ) res1 . printSchema () } // When we call show(x), this is what triggers the run println ( \"- Execute the Query\" ) time { res1 . show ( 5 ) } // Print the final row count println ( \"\\nGetting final row count, please wait...\" ) time { val rowcount = res1 . count () println ( f\"Epoch Conversion Processed : ( $rowcount %,d) Spots \" ) } } // END - Main CLass Python def pandas_convert_csv ( csvfile ): \"\"\"Convert CSV file using parquet_type compression\"\"\" file_name = os . path . basename ( csvfile ) clear () print ( \" \\n Pandas CSV Conversion Method\" ) print ( f \"Parquet Compression Types : { parquet_types } \" ) print ( \"Sit back and relax, this takes a while!! \\n \" ) print ( f '* Reading file : { file_name } ' ) start = time . time () df = pd . read_csv ( csvfile , dtype = spot_dtype , names = column_names , header = None ) rc = df . shape [ 0 ] print ( f \"* Spot Count : { rc : , } \" ) end = time . time () print ( f \"* File Size : { round ( get_file_size ( csvfile , 'csv' ), 2 ) } MB\" ) print ( f \"* Elapsed Time : { round (( end - start ), 3 ) } sec\" ) for f in parquet_types : compression_type = str ( f . upper ()) file_name = csvfile . replace ( 'csv' , f . lower ()) if compression_type == \"PARQUET\" : comp_type = \"NONE\" else : comp_type = compression_type . upper () print ( f ' \\n * Converting CSV to -> { f . lower () } ' ) start = time . time () df . to_parquet ( file_name , compression = str ( comp_type . upper ())) end = time . time () time . sleep ( sleep_time ) # prevent seg-fault on reads that are too quick print ( f \"* File Size : { round ( get_file_size ( csvfile , comp_type ), 2 ) } MB\" ) print ( f \"* Elapsed Time : { round (( end - start ), 3 ) } sec\" ) Java /** * * Static Method: Unzip a file to a path location * */ private static void UnzipFile ( String zipFilePath , String destDir ) { File dir = new File ( destDir ); if ( ! dir . exists ()) { dir . mkdirs (); } FileInputStream fis ; byte [] buffer = new byte [ 1024 ] ; try { fis = new FileInputStream ( zipFilePath ); ZipInputStream zis = new ZipInputStream ( fis ); ZipEntry ze = zis . getNextEntry (); // outer-loop while ( ze != null ) { String fileName = ze . getName (); File newFile = new File ( destDir + File . separator + fileName ); System . out . println ( \"* Unzipping to \" + newFile . getAbsolutePath ()); new File ( newFile . getParent ()). mkdirs (); FileOutputStream fos = new FileOutputStream ( newFile ); int len ; // inner-loop while (( len = zis . read ( buffer )) > 0 ) { fos . write ( buffer , 0 , len ); } fos . close (); //close this ZipEntry zis . closeEntry (); ze = zis . getNextEntry (); } // close the ZipEntry zis . closeEntry (); zis . close (); fis . close (); } catch ( IOException e ) { e . printStackTrace (); System . exit ( 2 ); } } // END - UnzipFile method WSPR Analytics is Apache 2.0 licensed code.","title":"Home"},{"location":"#wspr-analytics","text":"In the early days (March 2008), WSPR Spots measured in the hundreds of thousands per month. Today, that number has increased to over 75+ Million per month and shows no sign of abatement. By any reasonable definition, it is safe to say that WSPR has entered the realm of Big Data .","title":"WSPR Analytics"},{"location":"#project-goal","text":"The goal of this project is to provide a set of tools to download, manage, transform and query WSPR DataSets using modern Big Data frameworks.","title":"Project Goal"},{"location":"#folder-descriptions","text":"Several frameworks are used in this repository. The following matrix provides a short description of each, and their intended purpose. docs - Python, MkDocs for repository documentation java - Java, Maven, and SBT apps for RDD and Avro examples notebooks - Jupyter Notebooks for basic test and visualization pyspark - Python, PyArrow scripts that interact with CSV and Parquet files spark -Scala programs to perform tasks wsprdaemon - Python, Scala, Psql utilities related to the WSPR Daemon project wsprana - Python, (soon to be retired) Pay close attention to the README files as they lay out how to setup the tools needed to run their respective scripts or application.","title":"Folder Descriptions"},{"location":"#basic-tool-requirements","text":"NOTE: During development, the wsprana package is not intended for pip installaiton yet, but will be.It should be checked out and run from source at present. You must have Python, Java, PySpark/Spark available from the command line. Java openjdk version 1.8.0_275 or later Python 3.7+ PySpark from PyPi Apache Arrow 2.0+ Scala 2.12+ Spark 3.0.1 PostgreSQL Database (local, remote, Docker, etc) An easy way (on Linux / MacOS) to manage Java, Spark, Scala and SBT is through an management tool called sdkman . This tool allows one to install virtually any combination of tools you need without affecting your root file system. With the exception of Python, All the above requirements can be installed and managed via sdkman . For Python, the recomendation is to use Anaconda Python , the full version, as it provides all the analytics tooling you'll need for this project and more.","title":"Basic Tool Requirements"},{"location":"#data-sources-and-processing","text":"The primary data source will be the monthly WSPRNet Archives . At present, there is no plan to pull nightly updates. That could change if a reasonble API is identified. The WSPR CSV tools will be used to convert the raw CSV files into a format better suited for parallel processing, namely, Parquet . Read speeds, storage footprints, and ingestion improve dramativaly with this storage format. However, there is a drawback, one cannot simply view a binary file as they can with raw text files. The original CSV will remain in place, but all bulk processing will be pulled from Parquet . During these transformations is where PyArrow + PySpark will earn it's keep.","title":"Data Sources and Processing"},{"location":"#persistant-storage","text":"A PostgreSQL database server will be needed. There are many ways to perform this installation (local, remote, Dockerize PostgreSQL , PostgreSQL with Vagrant , etc). Whichever method you chose, it will be used extensively by many of the apps and scripts.","title":"Persistant Storage"},{"location":"#distribution-tabs","text":"In many of the installation sections, you will see Tabs for a particular distribution. Clicking on the desired tab will render the command or content relevant to that distribution. NOTE: These are just examples, and not intended for actual use. Alpine Update the package list apk update Add a package apk add openssh apk add openssh opentp vim Ubuntu Upgrade the host System Packages. # Run the following command sudo apt-get update && sudo apt-get upgrade Mint Install a pre-requesite package for VirtualBox. # Run the following command sudo apt-get update sudo apt-get install dkms Fedora a. Update your fedora release sudo dnf upgrade --refresh b. Install a plugin sudo dnf install dnf-plugin-system-upgrade c. Download upgraded packages sudo dnf system-upgrade download --refresh --releasever = 33 Windows Lets not and say we did! REM Run the following command echo Spark runs better on Linux. echo Please consider running Spark apps in echo VirtualBox if your host os is Windows!!","title":"Distribution Tabs"},{"location":"#super-fencing","text":"In many examples you may see multiple tabs relating to a particular code-block. Clicking on each tab shows the syntax for the stated language. This is the same behaviour as with Distribution Tabs C #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } C++ #include <iostream> int main ( void ) { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; } Scala /** * * Convert Epoch in from WSPRnet CSV files * */ def main ( args : Array [ String ]) : Unit = { val debug : Boolean = false // make Java's log4j warnings be quiet PropertyConfigurator . configure ( \"log4j/log4j.properties\" ) // IMPORTANT: When converting EPOCH times, you must do so with the // to_utc_timestamp method. This requires telling the system what Zone // your computer is in (the one doing the conversion) in order to get // the correct unix time. val z = ZoneId . systemDefault () val zoneId = z . getId println ( \"Process Steps For Processing A CSV File\" ) println ( \"- Create a Spark Session\" ) // Create the SPark Session val spark : SparkSession = SparkSession . builder () . appName ( \"Read CSV and Show Schema\" ) . master ( \"local[16]\" ) . getOrCreate () // Add Type-Safe Schema println ( \"- Create the Spot Schema\" ) val spotSchema = new StructType () . add ( \"SpotID\" , LongType , nullable = false ) . add ( \"Timestamp\" , IntegerType , nullable = false ) . add ( \"Reporter\" , StringType , nullable = false ) . add ( \"RxGrid\" , StringType , nullable = false ) . add ( \"SNR\" , ByteType , nullable = false ) . add ( \"Frequency\" , DoubleType , nullable = false ) . add ( \"CallSign\" , StringType , nullable = false ) . add ( \"Grid\" , StringType , nullable = false ) . add ( \"Power\" , ByteType , nullable = false ) . add ( \"Drift\" , ByteType , nullable = false ) . add ( \"Distance\" , ShortType , nullable = false ) . add ( \"Azimuth\" , ByteType , nullable = false ) . add ( \"Band\" , ByteType , nullable = false ) . add ( \"Version\" , StringType , nullable = true ) . add ( \"Code\" , ByteType , nullable = true ) // Create the Spark DataSet ( using small 100K csv ) println ( \"- Read the CSV file into a DataSet\" ) import spark.implicits._ val ds = spark . read . option ( \"delimiter\" , \",\" ) . option ( \"header\" , \"false\" ) . schema ( spotSchema ) . csv ( path = \"data/spots-2020-02-100K.csv\" ) . as [ RawSpot ] println ( \"- Select the column we want to process\" ) // Filter the data set val res = ds . select ( \"*\" ) . withColumn ( \"x_TimeStamp\" , date_format ( col ( \"TimeStamp\" ) . cast ( DataTypes . TimestampType ), \"yyyy-MM-dd HH:mm:ss\" )) // only print the schema in Debug Mode if ( debug ) { res . printSchema () } // See note above about ZoneId, it's important! println ( \"- Setup Epoh Conversion\" ) val res1 = res . select ( \"*\" ) . withColumn ( \"x_timestamp\" , to_utc_timestamp ( col ( \"x_TimeStamp\" ), zoneId )) . withColumn ( \"x_date\" , to_date ( col ( \"x_TimeStamp\" ))) . withColumn ( \"x_year\" , year ( col ( \"x_TimeStamp\" )). cast ( ShortType )) . withColumn ( \"x_month\" , month ( col ( \"x_TimeStamp\" )). cast ( ByteType )) . withColumn ( \"x_day\" , dayofmonth ( col ( \"x_TimeStamp\" )). cast ( ByteType )) . withColumn ( \"x_hour\" , hour ( col ( \"x_TimeStamp\" )). cast ( ByteType )) . withColumn ( \"x_minute\" , minute ( col ( \"x_TimeStamp\" )). cast ( ByteType )) // only print the schema in Debug Mode if ( debug ) { println ( \"- Print Res1 Schema\" ) res1 . printSchema () } // When we call show(x), this is what triggers the run println ( \"- Execute the Query\" ) time { res1 . show ( 5 ) } // Print the final row count println ( \"\\nGetting final row count, please wait...\" ) time { val rowcount = res1 . count () println ( f\"Epoch Conversion Processed : ( $rowcount %,d) Spots \" ) } } // END - Main CLass Python def pandas_convert_csv ( csvfile ): \"\"\"Convert CSV file using parquet_type compression\"\"\" file_name = os . path . basename ( csvfile ) clear () print ( \" \\n Pandas CSV Conversion Method\" ) print ( f \"Parquet Compression Types : { parquet_types } \" ) print ( \"Sit back and relax, this takes a while!! \\n \" ) print ( f '* Reading file : { file_name } ' ) start = time . time () df = pd . read_csv ( csvfile , dtype = spot_dtype , names = column_names , header = None ) rc = df . shape [ 0 ] print ( f \"* Spot Count : { rc : , } \" ) end = time . time () print ( f \"* File Size : { round ( get_file_size ( csvfile , 'csv' ), 2 ) } MB\" ) print ( f \"* Elapsed Time : { round (( end - start ), 3 ) } sec\" ) for f in parquet_types : compression_type = str ( f . upper ()) file_name = csvfile . replace ( 'csv' , f . lower ()) if compression_type == \"PARQUET\" : comp_type = \"NONE\" else : comp_type = compression_type . upper () print ( f ' \\n * Converting CSV to -> { f . lower () } ' ) start = time . time () df . to_parquet ( file_name , compression = str ( comp_type . upper ())) end = time . time () time . sleep ( sleep_time ) # prevent seg-fault on reads that are too quick print ( f \"* File Size : { round ( get_file_size ( csvfile , comp_type ), 2 ) } MB\" ) print ( f \"* Elapsed Time : { round (( end - start ), 3 ) } sec\" ) Java /** * * Static Method: Unzip a file to a path location * */ private static void UnzipFile ( String zipFilePath , String destDir ) { File dir = new File ( destDir ); if ( ! dir . exists ()) { dir . mkdirs (); } FileInputStream fis ; byte [] buffer = new byte [ 1024 ] ; try { fis = new FileInputStream ( zipFilePath ); ZipInputStream zis = new ZipInputStream ( fis ); ZipEntry ze = zis . getNextEntry (); // outer-loop while ( ze != null ) { String fileName = ze . getName (); File newFile = new File ( destDir + File . separator + fileName ); System . out . println ( \"* Unzipping to \" + newFile . getAbsolutePath ()); new File ( newFile . getParent ()). mkdirs (); FileOutputStream fos = new FileOutputStream ( newFile ); int len ; // inner-loop while (( len = zis . read ( buffer )) > 0 ) { fos . write ( buffer , 0 , len ); } fos . close (); //close this ZipEntry zis . closeEntry (); ze = zis . getNextEntry (); } // close the ZipEntry zis . closeEntry (); zis . close (); fis . close (); } catch ( IOException e ) { e . printStackTrace (); System . exit ( 2 ); } } // END - UnzipFile method WSPR Analytics is Apache 2.0 licensed code.","title":"Super Fencing"},{"location":"notebooks/overview/","text":"Under Development This section will discuss using Jupyter Notebooks to visualize data.","title":"Overview"},{"location":"pyspark/basic-examples/","text":"Basic Examples \u00b6 The scripts in this directory are used for exploring the use of Spark , Apache Arrow , Scala and other Big Data frameworks to: Reduce WSPR CSV on-disk storage space Improve CSV read times and portability Improve query performance Produce a cost effective, yet scalable, analytics solution for WSPR data An additional dimension that could be explored is streaming. For those with suffecient energy, there are many frameworks that can be employed in this area: Apache Storm Spark Streaming Apache Flink ...and many others. All of the example scripts will be incorporated into a main application at some point, however, some will just be for testing. Additional tests will be done using Scala and / or Java for performance cvomparison. Initial tests will be run using Python, or more sustinct, PySpark , as it lends itself to rapid-deployment scenarios. NOTE: This exercise is not a full-blown scientific experiment. Rather, its more of a feasability or proof of concept exercise. Compute Constraints \u00b6 While there is no shortage of cloud-based Big Data solutions these days ( [AWS EMR][], Azure Synapse , Databricks , GCP , Google Big Query , etc), these tests will look at commonidty based on-prem resources in order to keep the overall cost to an acceptable level. In other words: a hefty workstation, a stack of Raspberry PI's, or an over-powered laptop should be capable of performing the tasks for the use case at hand. Compute Environment \u00b6 If you are on Windows 10 , I would highly recommended using Windows Subsystem Linux v2 . Doing so will make setting up Apache Spark much easier, and you can follow the directions below with mininal frustration. Java \u00b6 No matter which method of compute is employed, Java forms the foundation. I chose to use openjdk version 1.8.0_275 , however, Spark now fully supports Java-11 . I've also ran Java-14 , but prefer to stick with LTS versions becasue not all frameworks support the \"latest and greatest\" JVM. Python \u00b6 You need a Python environment, either from Anaconda Python or the standard Python installer and venv . Either way, you should run the tests in a virtual Python environment to keep these packages from interfering with your core operating system. SDK Manager \u00b6 While running tests, it can be frustraighting to manage the large matrix of frameworks. The use of sdkman can make quick work of this chore; it's up to you though. Running The Tests \u00b6 The first two tests will illustrate the speed increase by using Apache Arrow to process both CSV and Apache Parquet compressed binaries. The major difference between the native CSV file, and those compressed by the pandas_convert_csv.py script is, the CSV file has no schema header information accompanying the file whereas the Apache Parquet binaries have full headers and data-type structures defined. Using inferred-schema does work, but it is not without error. This is the primary reason to fully define the schemas beforehand. Pandas , buy default, is a single thread reader/writer. It can be made to pool, but it's not configured to do so out of the box. You will see the difference in reading raw CSV files during the compression run and that of Apache Arrow (e.g. PyArrow with Python) running in parallel doing the same task. To run the test scripts, perform the following: Download and extract a WSPR Archive Clone the wspr-analytics repository Set a path variable Install Python package requirments Runs the test scripts Here are the Bash commands I used to run the tests. # change the download location to whatever you prefer cd ~/Downloads wget -c http://wsprnet.org/archive/wsprspots-2020-02.csv.gz gzip -dk wsprspots-2020-02.csv.gz # set the path of the downloaded and extracted CSV file csvfile = $PWD /wsprspots-2020-02.csv # clone the repo git clone https://github.com/KI7MT/wspr-analytics.git # change directories and install dependencies cd ~/Downloads/wspr-analytics/experiments # NOTE: Be sure you are in a virtual environment \"before\" # installing Python packages python -m pip install -r requirement.txt # run the conversion script python pandas_convert_csv.py -f $csvfile Pandas Parquet Compression Test \u00b6 CSV Reader - Python Pandas Parquet Writer - Python Pandas To Parquet NOTE: Additional tests will be added to check the read/write speeds using Apache Arrow only while leaving Pandas out of the mix completely. The following data shows the results of converting a CSV file to several Apache Parquet formats. Substantial disk-space savings can be achived using these methods. Generally speaking, as the compression increases so does the length of time it takes to create the file. However, the disk-space savings are impressive. NOTE : Make note of the CSV Read Time while using Pandas in a Single Thead Disk space savings range from >= 5 : 1 to 8 : 1 depending on the compression you'd like to use. It's a substantial savings no matter which you chose. Pandas CSV Conversion Method Parquet Compression Types : [ 'SNAPPY' , 'LZ4' , 'ZSTD' , 'GZIP' , 'BROTLI' ] Sit back and relax, this takes a while !! * Reading file : wsprspots-2020-02.csv * Spot Count : 47 ,310,649 * File Size : 3780 .94 MB * Elapsed Time : 79 .448 sec * Converting CSV to -> snappy * File Size : 667 .07 MB * Elapsed Time : 29 .579 sec * Converting CSV to -> lz4 * File Size : 627 .88 MB * Elapsed Time : 29 .702 sec * Converting CSV to -> zstd * File Size : 520 .19 MB * Elapsed Time : 30 .778 sec * Converting CSV to -> gzip * File Size : 467 .55 MB * Elapsed Time : 107 .244 sec * Converting CSV to -> brotli * File Size : 446 .58 MB * Elapsed Time : 89 .529 sec NOTE : The File Sizes Are Approximated = ( file bytes / 1048576 ) Finished !! Apache Arrow (PyArrow) Read Test \u00b6 Reader - PyArrow The results below are from using Apache Arrow to read each of the compressed-file formats created above. No Spark nodes are deployed, only a single master with no special optimizations applied. Apache Arrow uses threads by default which is the main difference in using Pandas to read original CSV read. To say it's fast is an understatement. NOTE : Check the CSV Read Time below and compare it to the Pandas CSV read time from above; it's an 36 : 1 read-speed improvement or ( 80.32 sec v.s. 2.213 sec ) # You run the read script the same way python pyarrow_read.py -f $csvfile # Results: Running Read Tests Using Apache Arrow Compression Types : [ 'CSV' , 'SNAPPY' , 'ZSTD' , 'GZIP' , 'BROTLI' ] * Reading file : wsprspots-2020-02.csv * File Size : 3780 .94 MB * Elapsed Time : 2 .185 * Reading file : wsprspots-2020-02.snappy * File Size : 667 .07 MB * Elapsed Time : 2 .011 * Reading file : wsprspots-2020-02.zstd * File Size : 520 .19 MB * Elapsed Time : 1 .769 * Reading file : wsprspots-2020-02.gzip * File Size : 467 .55 MB * Elapsed Time : 3 .654 * Reading file : wsprspots-2020-02.brotli * File Size : 446 .58 MB * Elapsed Time : 2 .102 Finished !! Query Parquet File Using PySpark \u00b6 The next phase is to run a simple query using PySpark in in a distributed manner. As this is all happening on a single msater host node, it is more of a parralized action rather than distributed, but the results are far superior to what any single threaded implementation could achieve. See WSPR Query Notebook for details. The read speed is impressive. It takes ~1.5 seccond to read 47+ Million rows, and ~4.8 seconds to do a group by query. Row count speeds ( 0.87 sec ) are on par with using ( wc -l ) from a Linux shell. Now matter how one looks at this approach, it's a viable alternative to a raw csv read and process action. * Reading file ..: wsprspots-2020-02.parquet * File Size .....: 490 ,259,730 bytes compressed * Read Time .....: 1 .51834 sec * Counting Records * Record Count ..: 47 ,310,649 * Count Time ....: 0 .90268 sec * Running Group by Count Query and return the dataframe +--------+------+ | Reporter | count | +--------+------+ | DK6UG | 838081 | | OE9GHV | 690104 | | EA8BFK | 648670 | | KD2OM | 589003 | | KA7OEI-1 | 576788 | | K4RCG | 571445 | | KPH | 551690 | | K9AN | 480759 | | DF5FH | 480352 | | DJ9PC | 474211 | +--------+------+ only showing top 10 rows * Query Time ....: 4 .94034 sec","title":"Basic Examples"},{"location":"pyspark/basic-examples/#basic-examples","text":"The scripts in this directory are used for exploring the use of Spark , Apache Arrow , Scala and other Big Data frameworks to: Reduce WSPR CSV on-disk storage space Improve CSV read times and portability Improve query performance Produce a cost effective, yet scalable, analytics solution for WSPR data An additional dimension that could be explored is streaming. For those with suffecient energy, there are many frameworks that can be employed in this area: Apache Storm Spark Streaming Apache Flink ...and many others. All of the example scripts will be incorporated into a main application at some point, however, some will just be for testing. Additional tests will be done using Scala and / or Java for performance cvomparison. Initial tests will be run using Python, or more sustinct, PySpark , as it lends itself to rapid-deployment scenarios. NOTE: This exercise is not a full-blown scientific experiment. Rather, its more of a feasability or proof of concept exercise.","title":"Basic Examples"},{"location":"pyspark/basic-examples/#compute-constraints","text":"While there is no shortage of cloud-based Big Data solutions these days ( [AWS EMR][], Azure Synapse , Databricks , GCP , Google Big Query , etc), these tests will look at commonidty based on-prem resources in order to keep the overall cost to an acceptable level. In other words: a hefty workstation, a stack of Raspberry PI's, or an over-powered laptop should be capable of performing the tasks for the use case at hand.","title":"Compute Constraints"},{"location":"pyspark/basic-examples/#compute-environment","text":"If you are on Windows 10 , I would highly recommended using Windows Subsystem Linux v2 . Doing so will make setting up Apache Spark much easier, and you can follow the directions below with mininal frustration.","title":"Compute Environment"},{"location":"pyspark/basic-examples/#java","text":"No matter which method of compute is employed, Java forms the foundation. I chose to use openjdk version 1.8.0_275 , however, Spark now fully supports Java-11 . I've also ran Java-14 , but prefer to stick with LTS versions becasue not all frameworks support the \"latest and greatest\" JVM.","title":"Java"},{"location":"pyspark/basic-examples/#python","text":"You need a Python environment, either from Anaconda Python or the standard Python installer and venv . Either way, you should run the tests in a virtual Python environment to keep these packages from interfering with your core operating system.","title":"Python"},{"location":"pyspark/basic-examples/#sdk-manager","text":"While running tests, it can be frustraighting to manage the large matrix of frameworks. The use of sdkman can make quick work of this chore; it's up to you though.","title":"SDK Manager"},{"location":"pyspark/basic-examples/#running-the-tests","text":"The first two tests will illustrate the speed increase by using Apache Arrow to process both CSV and Apache Parquet compressed binaries. The major difference between the native CSV file, and those compressed by the pandas_convert_csv.py script is, the CSV file has no schema header information accompanying the file whereas the Apache Parquet binaries have full headers and data-type structures defined. Using inferred-schema does work, but it is not without error. This is the primary reason to fully define the schemas beforehand. Pandas , buy default, is a single thread reader/writer. It can be made to pool, but it's not configured to do so out of the box. You will see the difference in reading raw CSV files during the compression run and that of Apache Arrow (e.g. PyArrow with Python) running in parallel doing the same task. To run the test scripts, perform the following: Download and extract a WSPR Archive Clone the wspr-analytics repository Set a path variable Install Python package requirments Runs the test scripts Here are the Bash commands I used to run the tests. # change the download location to whatever you prefer cd ~/Downloads wget -c http://wsprnet.org/archive/wsprspots-2020-02.csv.gz gzip -dk wsprspots-2020-02.csv.gz # set the path of the downloaded and extracted CSV file csvfile = $PWD /wsprspots-2020-02.csv # clone the repo git clone https://github.com/KI7MT/wspr-analytics.git # change directories and install dependencies cd ~/Downloads/wspr-analytics/experiments # NOTE: Be sure you are in a virtual environment \"before\" # installing Python packages python -m pip install -r requirement.txt # run the conversion script python pandas_convert_csv.py -f $csvfile","title":"Running The Tests"},{"location":"pyspark/basic-examples/#pandas-parquet-compression-test","text":"CSV Reader - Python Pandas Parquet Writer - Python Pandas To Parquet NOTE: Additional tests will be added to check the read/write speeds using Apache Arrow only while leaving Pandas out of the mix completely. The following data shows the results of converting a CSV file to several Apache Parquet formats. Substantial disk-space savings can be achived using these methods. Generally speaking, as the compression increases so does the length of time it takes to create the file. However, the disk-space savings are impressive. NOTE : Make note of the CSV Read Time while using Pandas in a Single Thead Disk space savings range from >= 5 : 1 to 8 : 1 depending on the compression you'd like to use. It's a substantial savings no matter which you chose. Pandas CSV Conversion Method Parquet Compression Types : [ 'SNAPPY' , 'LZ4' , 'ZSTD' , 'GZIP' , 'BROTLI' ] Sit back and relax, this takes a while !! * Reading file : wsprspots-2020-02.csv * Spot Count : 47 ,310,649 * File Size : 3780 .94 MB * Elapsed Time : 79 .448 sec * Converting CSV to -> snappy * File Size : 667 .07 MB * Elapsed Time : 29 .579 sec * Converting CSV to -> lz4 * File Size : 627 .88 MB * Elapsed Time : 29 .702 sec * Converting CSV to -> zstd * File Size : 520 .19 MB * Elapsed Time : 30 .778 sec * Converting CSV to -> gzip * File Size : 467 .55 MB * Elapsed Time : 107 .244 sec * Converting CSV to -> brotli * File Size : 446 .58 MB * Elapsed Time : 89 .529 sec NOTE : The File Sizes Are Approximated = ( file bytes / 1048576 ) Finished !!","title":"Pandas Parquet Compression Test"},{"location":"pyspark/basic-examples/#apache-arrow-pyarrow-read-test","text":"Reader - PyArrow The results below are from using Apache Arrow to read each of the compressed-file formats created above. No Spark nodes are deployed, only a single master with no special optimizations applied. Apache Arrow uses threads by default which is the main difference in using Pandas to read original CSV read. To say it's fast is an understatement. NOTE : Check the CSV Read Time below and compare it to the Pandas CSV read time from above; it's an 36 : 1 read-speed improvement or ( 80.32 sec v.s. 2.213 sec ) # You run the read script the same way python pyarrow_read.py -f $csvfile # Results: Running Read Tests Using Apache Arrow Compression Types : [ 'CSV' , 'SNAPPY' , 'ZSTD' , 'GZIP' , 'BROTLI' ] * Reading file : wsprspots-2020-02.csv * File Size : 3780 .94 MB * Elapsed Time : 2 .185 * Reading file : wsprspots-2020-02.snappy * File Size : 667 .07 MB * Elapsed Time : 2 .011 * Reading file : wsprspots-2020-02.zstd * File Size : 520 .19 MB * Elapsed Time : 1 .769 * Reading file : wsprspots-2020-02.gzip * File Size : 467 .55 MB * Elapsed Time : 3 .654 * Reading file : wsprspots-2020-02.brotli * File Size : 446 .58 MB * Elapsed Time : 2 .102 Finished !!","title":"Apache Arrow (PyArrow) Read Test"},{"location":"pyspark/basic-examples/#query-parquet-file-using-pyspark","text":"The next phase is to run a simple query using PySpark in in a distributed manner. As this is all happening on a single msater host node, it is more of a parralized action rather than distributed, but the results are far superior to what any single threaded implementation could achieve. See WSPR Query Notebook for details. The read speed is impressive. It takes ~1.5 seccond to read 47+ Million rows, and ~4.8 seconds to do a group by query. Row count speeds ( 0.87 sec ) are on par with using ( wc -l ) from a Linux shell. Now matter how one looks at this approach, it's a viable alternative to a raw csv read and process action. * Reading file ..: wsprspots-2020-02.parquet * File Size .....: 490 ,259,730 bytes compressed * Read Time .....: 1 .51834 sec * Counting Records * Record Count ..: 47 ,310,649 * Count Time ....: 0 .90268 sec * Running Group by Count Query and return the dataframe +--------+------+ | Reporter | count | +--------+------+ | DK6UG | 838081 | | OE9GHV | 690104 | | EA8BFK | 648670 | | KD2OM | 589003 | | KA7OEI-1 | 576788 | | K4RCG | 571445 | | KPH | 551690 | | K9AN | 480759 | | DF5FH | 480352 | | DJ9PC | 474211 | +--------+------+ only showing top 10 rows * Query Time ....: 4 .94034 sec","title":"Query Parquet File Using PySpark"},{"location":"pyspark/overview/","text":"Apache Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for incremental computation and stream processing. -- Apache Spark Project Apache Spark is written in a programming language called Scala , which runs on a Java Virtual Machine (JVM). To allow for an easy integration between Scala and Python, the programming community created a tool set (libraries/modules/scripts) called PySpark . Other languages such as Ruby, R, and Java also have bindings that support or work with the Spark framework. It can get a bit confusing trying to distinguish the manay moving parts of the Spark echosystem. To make it a bit easier, just remember Spark is the foundation, and PySpark in merely an integration to that foundation. For the purposes of the WSPR Analytics project, we'll being using PySpark as a easy gateway to Jupyter Notebooks in a distributive manner. As with Spark , PySpark has it's own shell when not being forwarded to Jupyter Notebooks . However, both still rely on Spark as the compute engine, and in this case, Python is the interface via the command-line rather than web page. You can see by the shell below, I was running Anaconda Python v3.7.9 from with Spark v3.0.1. You can also see that the pyspark shell creates a SparkContext for us upon entry. This will be better understood when executng example scripts. Python 3 .7.9 ( default, Aug 31 2020 , 12 :42:55 ) [ GCC 7 .3.0 ] :: Anaconda, Inc. on linux Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information. Welcome to ____ __ / __/__ ___ _____/ /__ _ \\ \\/ _ \\/ _ ` / __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 3.0.1 /_/ Using Python version 3.7.9 (default, Aug 31 2020 12:42:55) SparkSession available as ' spark '. >>> spark.version ' 3 .0.1 ' >>> sc.version ' 3 .0.1 ' Additional Resources \u00b6 There are many high-quailty sites that cover all aspects of PySpark in great detail: Databricks - The original creator of Spark Learning Spark 2 nd Edition , also from Databricks, is a great book PySpark Docs covers SQL, ML, Streaming and more Spark itself from the Apache Foundation PySpark at JavaPoint is a great resource for all things coding","title":"Overview"},{"location":"pyspark/overview/#additional-resources","text":"There are many high-quailty sites that cover all aspects of PySpark in great detail: Databricks - The original creator of Spark Learning Spark 2 nd Edition , also from Databricks, is a great book PySpark Docs covers SQL, ML, Streaming and more Spark itself from the Apache Foundation PySpark at JavaPoint is a great resource for all things coding","title":"Additional Resources"},{"location":"rscripts/overview/","text":"Under Development This section will be the overview page to R-Scripts.","title":"Overview"},{"location":"spark/convert-csv-parquet/","text":"Convert CSV File to Parquet \u00b6 This is a sample Application using Scala that performs the following: Reads a WSPRnet CSV from an input path e.g /data/wspr/csv.wsprspots-2020-02.csv Creates a Parquet file set to an output path e.g /data/wspr/parquet/2020/02 If you re-run the script, the output Parquet directory will be overwritten. File Specs \u00b6 The specs on the test file are: Test File : wsprspots-2020-02.csv Rows : 47,310,649 spots File Size Decompressed : 3.964 GB Build and Run \u00b6 Run the following commands in order, and check your results. # # All commands are run from a terminal # # change the download location to whatever you prefer cd ~/Downloads wget -c http://wsprnet.org/archive/wsprspots-2020-02.csv.gz gzip -dk wsprspots-2020-02.csv.gz # set the path of the downloaded and extracted CSV file infile = $PWD /wsprspots-2020-02.csv outdir = $PWD /wspr/parquet/2020/02 # clone the repo git clone https://github.com/KI7MT/wspr-analytics.git # change directories and build the assembly cd ./wspr-analytics/scala/ConvertCsvToParquet # clean and build sbt clean assembly # Run the following command # NOTE : set local[16] to half of your total CPU count. spark-submit --master local [ 16 ] target/scala-2.12/ConvertCsvToParquet-assembly-1.0.jar $infile $outdir Results \u00b6 You should get results similar to the following: Out Directory $PWD/wspr/parquet/2020/02 Compressed Size ~615 MB on-disk Process Time was =< 21sec The example below will differ somewhat due to my CSV input and output choices. NOTE The time it takes will depend on your system resources (CPU, RAM, etc) Object : ConvertCsvToParquet Process File : /data/wspr/raw/csv/wsprspots-2020-02.csv File Out Path : /data/wspr/raw/parquet/2020/02 Tiimestame : 2020 -12-28 T 04 :36:29.941 Description : Convert CSV to Parquet Process Steps to Create Parquet File ( s ) - Create a Spark Session - Create the Spot Schema - Read the CSV file into a DataSet - Write Parquet File ( s ) , please wait... Elapsed Time : 20 .456 sec Finished","title":"Convert To Parquet"},{"location":"spark/convert-csv-parquet/#convert-csv-file-to-parquet","text":"This is a sample Application using Scala that performs the following: Reads a WSPRnet CSV from an input path e.g /data/wspr/csv.wsprspots-2020-02.csv Creates a Parquet file set to an output path e.g /data/wspr/parquet/2020/02 If you re-run the script, the output Parquet directory will be overwritten.","title":"Convert CSV File to Parquet"},{"location":"spark/convert-csv-parquet/#file-specs","text":"The specs on the test file are: Test File : wsprspots-2020-02.csv Rows : 47,310,649 spots File Size Decompressed : 3.964 GB","title":"File Specs"},{"location":"spark/convert-csv-parquet/#build-and-run","text":"Run the following commands in order, and check your results. # # All commands are run from a terminal # # change the download location to whatever you prefer cd ~/Downloads wget -c http://wsprnet.org/archive/wsprspots-2020-02.csv.gz gzip -dk wsprspots-2020-02.csv.gz # set the path of the downloaded and extracted CSV file infile = $PWD /wsprspots-2020-02.csv outdir = $PWD /wspr/parquet/2020/02 # clone the repo git clone https://github.com/KI7MT/wspr-analytics.git # change directories and build the assembly cd ./wspr-analytics/scala/ConvertCsvToParquet # clean and build sbt clean assembly # Run the following command # NOTE : set local[16] to half of your total CPU count. spark-submit --master local [ 16 ] target/scala-2.12/ConvertCsvToParquet-assembly-1.0.jar $infile $outdir","title":"Build and Run"},{"location":"spark/convert-csv-parquet/#results","text":"You should get results similar to the following: Out Directory $PWD/wspr/parquet/2020/02 Compressed Size ~615 MB on-disk Process Time was =< 21sec The example below will differ somewhat due to my CSV input and output choices. NOTE The time it takes will depend on your system resources (CPU, RAM, etc) Object : ConvertCsvToParquet Process File : /data/wspr/raw/csv/wsprspots-2020-02.csv File Out Path : /data/wspr/raw/parquet/2020/02 Tiimestame : 2020 -12-28 T 04 :36:29.941 Description : Convert CSV to Parquet Process Steps to Create Parquet File ( s ) - Create a Spark Session - Create the Spot Schema - Read the CSV file into a DataSet - Write Parquet File ( s ) , please wait... Elapsed Time : 20 .456 sec Finished","title":"Results"},{"location":"spark/overview/","text":"Apache Spark\u2122 is a unified analytics engine for large-scale data processing. Apache Spark Project Framework Requirements \u00b6 You must have Java, Scala, Spark and SBT available from the command line for all Scala Projects listed in this section. Java openjdk version 1.8.0_275 or later Scala Version 2.12.12 Spark 3.0.1 SBT Tools 1.4.5 An easy way (on Linux / MacOS) to mange Java, Spark, Scala and SBT is through an management tool called sdkman . This tool allows one to install virtually any combination of tools you need without affecting your root file system. All the above requirements can be installed and managed via sdkman . If you can't run : spark-shell and have it present the REPL, see Installing Spark NOTE: Your version of Java and Scala may be different. ( wsprana ) ki7mt @ 3950 X :~ $ spark - shell Spark context Web UI available at http :// localhost : 4040 Spark context available as ' sc ' ( master = local [ * ], app id = local - 1609375750950 ). Spark session available as 'spark ' . Welcome to ____ __ / __/__ ___ _____ / / __ _ \\ \\/ _ \\/ _ ` / __ / '_/ / ___ / . __/\\ _ , _ / _ / / _ /\\ _ \\ version 3.0 . 1 / _ / Using Scala version 2.12 . 10 ( OpenJDK 64 - Bit Server VM , Java 1.8 . 0 _275 ) Type in expressions to have them evaluated . Type : help for more information. scala> Read and Query DataSet \u00b6 // Read the CSV into the DataSet println ( \"- Reading CSV into DataSet\" ) import spark.implicits._ val reporterDS = spark . read . schema ( reporterSchema ) . csv ( csvfile ) . as [ Reporter ] // Print results from the dataset println ( \"- Query Execution\\n\" ) time { sortedResults . show ( 10 )}","title":"Overview"},{"location":"spark/overview/#framework-requirements","text":"You must have Java, Scala, Spark and SBT available from the command line for all Scala Projects listed in this section. Java openjdk version 1.8.0_275 or later Scala Version 2.12.12 Spark 3.0.1 SBT Tools 1.4.5 An easy way (on Linux / MacOS) to mange Java, Spark, Scala and SBT is through an management tool called sdkman . This tool allows one to install virtually any combination of tools you need without affecting your root file system. All the above requirements can be installed and managed via sdkman . If you can't run : spark-shell and have it present the REPL, see Installing Spark NOTE: Your version of Java and Scala may be different. ( wsprana ) ki7mt @ 3950 X :~ $ spark - shell Spark context Web UI available at http :// localhost : 4040 Spark context available as ' sc ' ( master = local [ * ], app id = local - 1609375750950 ). Spark session available as 'spark ' . Welcome to ____ __ / __/__ ___ _____ / / __ _ \\ \\/ _ \\/ _ ` / __ / '_/ / ___ / . __/\\ _ , _ / _ / / _ /\\ _ \\ version 3.0 . 1 / _ / Using Scala version 2.12 . 10 ( OpenJDK 64 - Bit Server VM , Java 1.8 . 0 _275 ) Type in expressions to have them evaluated . Type : help for more information. scala>","title":"Framework Requirements"},{"location":"spark/overview/#read-and-query-dataset","text":"// Read the CSV into the DataSet println ( \"- Reading CSV into DataSet\" ) import spark.implicits._ val reporterDS = spark . read . schema ( reporterSchema ) . csv ( csvfile ) . as [ Reporter ] // Print results from the dataset println ( \"- Query Execution\\n\" ) time { sortedResults . show ( 10 )}","title":"Read and Query DataSet"},{"location":"spark/query-with-parquet/","text":"Query Column Using Parquet Files \u00b6 This is a sample Application using Scala that performs the following: Reads a Parquet folder specificed by the user e.g. /data/wspr/raw/parquet/2020/02 Performs a Query Count on a column specified by the user e.g. Reporter Reports GoubyBy Count and returns the Top (10) File Specs \u00b6 The specs on the test file are: Test File : wsprspots-2020-02.csv Rows : 47,310,649 spots File Size Decompressed : 3.964 GB Build and Run \u00b6 Run the following commands in order, and check your results. # # All commands are run from a terminal # # NOTE: You must have previously run ConvertCvsToParquet before # using this app, as it is looking for a Parquet folder, # not a CSV file. # set the path to where you created the Parquet partition inFolder = \"/data/wspr/raw/parquet/2020/02\" # set the colum you wish to GroupBy and Count column = \"Reporter\" # clone the repo git clone https://github.com/KI7MT/wspr-analytics.git # change directories and build the assembly cd ./wspr-analytics/scala/QueryColumnParquet # clean and build sbt clean assembly # Run the following command # NOTE : set local[8] to half of your total CPU count. spark-submit --master local [ 8 ] target/scala-2.12/QueryColumnParquet-assembly-1.0.jar $inFolder $column Results \u00b6 You should get results similar to the following: NOTE The time it takes will depend on your system resources (CPU, RAM, etc) Application : QueryColumnParquet Folder : /data/wspr/raw/parquet/2020/02 Column : Reporter Tiimestame : 2020 -12-28T07:36:35.389 Description : Query Column and Count using Parquet Folders Process Steps to Query Reporter from Parquet Files ( s ) - Create a Spark Session - Read Parquet File ( s ) - Select Reporter - GroupBy and Count Reporter - Sort Reporter Descending - Execute the Query +--------+------+ | Reporter | count | +--------+------+ | DK6UG | 838081 | | OE9GHV | 690104 | | EA8BFK | 648670 | | KD2OM | 589003 | | KA7OEI-1 | 576788 | | K4RCG | 571445 | | KPH | 551690 | | K9AN | 480759 | | DF5FH | 480352 | | DJ9PC | 474211 | +--------+------+ only showing top 10 rows Elapsed Time : 2 .843 sec Query by Reporter \u00b6 For Comparrison, here is the data from 2020-11 . November 2020 had over 70 Million reports. Application : QueryColumnParquet Folder : /data/wspr/raw/parquet/2020/11 Column : Reporter Tiimestame : 2020 -12-28 T 07 :42:09.275 Description : Query Column and Count using Parquet Folders Process Steps to Query Reporter from Parquet Files ( s ) - Create a Spark Session - Read Parquet File ( s ) - Select Reporter - GroupBy and Count Reporter - Sort Reporter Descending - Execute the Query +--------+-------+ | Reporter | count | +--------+-------+ | EA8BFK | 1120739 | | OE9GHV | 1103335 | | WA2TP | 847124 | | KD2OM | 834896 | | IW2NKE | 818315 | | LX1DQ | 803347 | | DK6UG | 794675 | | KA7OEI-1 | 748356 | | ON5KQ | 744727 | | OE9HLH | 733580 | +--------+-------+ only showing top 10 rows Elapsed Time : 2 .8 sec Query by CallSign \u00b6 This query is from the CallSign column. Application : QueryColumnParquet Folder : /data/wspr/raw/parquet/2020/11 Column : CallSign Tiimestame : 2020 -12-28 T 07 :44:16.711 Description : Query Column and Count using Parquet Folders Process Steps to Query CallSign from Parquet Files ( s ) - Create a Spark Session - Read Parquet File ( s ) - Select CallSign - GroupBy and Count CallSign - Sort CallSign Descending - Execute the Query +--------+------+ | CallSign | count | +--------+------+ | DK2DB | 662784 | | K4APC | 600895 | | ON7KO | 557477 | | KD6RF | 541418 | | WA4KFZ | 531006 | | W6LVP | 466346 | | DL6NL | 438741 | | N8VIM | 438260 | | DK8JP | 403667 | | G0CCL | 386570 | +--------+------+ only showing top 10 rows Elapsed Time : 2 .445 sec Query by Version \u00b6 Application : QueryColumnParquet Folder : /data/wspr/raw/parquet/2020/11 Column : Version Tiimestame : 2020 -12-28 T 07 :50:17.725 Description : Query Column and Count using Parquet Folders Process Steps to Query Version from Parquet Files ( s ) - Create a Spark Session - Read Parquet File ( s ) - Select Version - GroupBy and Count Version - Sort Version Descending - Execute the Query +---------+--------+ | Version | count | +---------+--------+ | null | 27015499 | | 1 .3 Kiwi | 13207118 | | 2 .2.2 | 11199415 | | 2 .1.2 | 5906973 | | 2 .3.0-rc1 | 2952306 | | 2 .1.0 | 2602903 | | 2 .3.0-rc2 | 2366520 | | 0 .9_r4178 | 2033622 | | 2 .2.1 | 2027954 | | 2 .0.0 | 1482597 | +---------+--------+ only showing top 10 rows Elapsed Time : 2 .492 sec","title":"Query With Parquet"},{"location":"spark/query-with-parquet/#query-column-using-parquet-files","text":"This is a sample Application using Scala that performs the following: Reads a Parquet folder specificed by the user e.g. /data/wspr/raw/parquet/2020/02 Performs a Query Count on a column specified by the user e.g. Reporter Reports GoubyBy Count and returns the Top (10)","title":"Query Column Using Parquet Files"},{"location":"spark/query-with-parquet/#file-specs","text":"The specs on the test file are: Test File : wsprspots-2020-02.csv Rows : 47,310,649 spots File Size Decompressed : 3.964 GB","title":"File Specs"},{"location":"spark/query-with-parquet/#build-and-run","text":"Run the following commands in order, and check your results. # # All commands are run from a terminal # # NOTE: You must have previously run ConvertCvsToParquet before # using this app, as it is looking for a Parquet folder, # not a CSV file. # set the path to where you created the Parquet partition inFolder = \"/data/wspr/raw/parquet/2020/02\" # set the colum you wish to GroupBy and Count column = \"Reporter\" # clone the repo git clone https://github.com/KI7MT/wspr-analytics.git # change directories and build the assembly cd ./wspr-analytics/scala/QueryColumnParquet # clean and build sbt clean assembly # Run the following command # NOTE : set local[8] to half of your total CPU count. spark-submit --master local [ 8 ] target/scala-2.12/QueryColumnParquet-assembly-1.0.jar $inFolder $column","title":"Build and Run"},{"location":"spark/query-with-parquet/#results","text":"You should get results similar to the following: NOTE The time it takes will depend on your system resources (CPU, RAM, etc) Application : QueryColumnParquet Folder : /data/wspr/raw/parquet/2020/02 Column : Reporter Tiimestame : 2020 -12-28T07:36:35.389 Description : Query Column and Count using Parquet Folders Process Steps to Query Reporter from Parquet Files ( s ) - Create a Spark Session - Read Parquet File ( s ) - Select Reporter - GroupBy and Count Reporter - Sort Reporter Descending - Execute the Query +--------+------+ | Reporter | count | +--------+------+ | DK6UG | 838081 | | OE9GHV | 690104 | | EA8BFK | 648670 | | KD2OM | 589003 | | KA7OEI-1 | 576788 | | K4RCG | 571445 | | KPH | 551690 | | K9AN | 480759 | | DF5FH | 480352 | | DJ9PC | 474211 | +--------+------+ only showing top 10 rows Elapsed Time : 2 .843 sec","title":"Results"},{"location":"spark/query-with-parquet/#query-by-reporter","text":"For Comparrison, here is the data from 2020-11 . November 2020 had over 70 Million reports. Application : QueryColumnParquet Folder : /data/wspr/raw/parquet/2020/11 Column : Reporter Tiimestame : 2020 -12-28 T 07 :42:09.275 Description : Query Column and Count using Parquet Folders Process Steps to Query Reporter from Parquet Files ( s ) - Create a Spark Session - Read Parquet File ( s ) - Select Reporter - GroupBy and Count Reporter - Sort Reporter Descending - Execute the Query +--------+-------+ | Reporter | count | +--------+-------+ | EA8BFK | 1120739 | | OE9GHV | 1103335 | | WA2TP | 847124 | | KD2OM | 834896 | | IW2NKE | 818315 | | LX1DQ | 803347 | | DK6UG | 794675 | | KA7OEI-1 | 748356 | | ON5KQ | 744727 | | OE9HLH | 733580 | +--------+-------+ only showing top 10 rows Elapsed Time : 2 .8 sec","title":"Query by Reporter"},{"location":"spark/query-with-parquet/#query-by-callsign","text":"This query is from the CallSign column. Application : QueryColumnParquet Folder : /data/wspr/raw/parquet/2020/11 Column : CallSign Tiimestame : 2020 -12-28 T 07 :44:16.711 Description : Query Column and Count using Parquet Folders Process Steps to Query CallSign from Parquet Files ( s ) - Create a Spark Session - Read Parquet File ( s ) - Select CallSign - GroupBy and Count CallSign - Sort CallSign Descending - Execute the Query +--------+------+ | CallSign | count | +--------+------+ | DK2DB | 662784 | | K4APC | 600895 | | ON7KO | 557477 | | KD6RF | 541418 | | WA4KFZ | 531006 | | W6LVP | 466346 | | DL6NL | 438741 | | N8VIM | 438260 | | DK8JP | 403667 | | G0CCL | 386570 | +--------+------+ only showing top 10 rows Elapsed Time : 2 .445 sec","title":"Query by CallSign"},{"location":"spark/query-with-parquet/#query-by-version","text":"Application : QueryColumnParquet Folder : /data/wspr/raw/parquet/2020/11 Column : Version Tiimestame : 2020 -12-28 T 07 :50:17.725 Description : Query Column and Count using Parquet Folders Process Steps to Query Version from Parquet Files ( s ) - Create a Spark Session - Read Parquet File ( s ) - Select Version - GroupBy and Count Version - Sort Version Descending - Execute the Query +---------+--------+ | Version | count | +---------+--------+ | null | 27015499 | | 1 .3 Kiwi | 13207118 | | 2 .2.2 | 11199415 | | 2 .1.2 | 5906973 | | 2 .3.0-rc1 | 2952306 | | 2 .1.0 | 2602903 | | 2 .3.0-rc2 | 2366520 | | 0 .9_r4178 | 2033622 | | 2 .2.1 | 2027954 | | 2 .0.0 | 1482597 | +---------+--------+ only showing top 10 rows Elapsed Time : 2 .492 sec","title":"Query by Version"},{"location":"spark/spark-examples/","text":"Spark Examples \u00b6 Under Development This projest folder will be using IntelliJ IDEA to test various script, apps and snippets using Scala","title":"Spark Examples"},{"location":"spark/spark-examples/#spark-examples","text":"Under Development This projest folder will be using IntelliJ IDEA to test various script, apps and snippets using Scala","title":"Spark Examples"},{"location":"spark/top-ten-reporters/","text":"Top Ten Reporters \u00b6 This is a sample application using Scala that performs the following: Reads the Original CSV into a Spark DataFrame Performas a Query Count on Reporters Ordered Descending Reports Top (10) by spot count File Specs \u00b6 The specs on the test file are: Test File : wsprspots-2020-02.csv Rows : 47,310,649 spots File Size Decompressed : 3.964 GB If you use a different archive, make sure to you pass the relative location to the script when running. Build and Run \u00b6 Run the following commands in order, and check your results. # # All commands are run from a terminal # cd ~/Downloads wget -c http://wsprnet.org/archive/wsprspots-2020-02.csv.gz gzip -dk wsprspots-2020-02.csv.gz # set the path of the downloaded and extracted CSV file csvfile = $PWD /wsprspots-2020-02.csv # clone the repo git clone https://github.com/KI7MT/wspr-analytics.git # change directories and build the assembly cd ./wspr-analytics/scala/TopTenReporters # clean and build sbt clean assembly # Runs the following command spark-submit --master local [ 8 ] target/scala-2.12/TopTenReporter-assembly-1.0.jar $csvfile Results \u00b6 You should get results similar to the following: NOTE The time it takes will depend on your system resources (CPU, RAM, etc) Application : TopTenReporter Process File : wsprspots-2020-02.csv Tiimestame : 2020 -12-27 T 02 :36:01.265 Description : Returns the Top Ten Reporters Grouped by Count Process Steps for this application - Creating the Schema - Reading CSV into DataSet - Selecting Reporters - GroupBy and Count Reporters - Sort Reporters Descending - Query Execution +--------+------+ | Reporter | count | +--------+------+ | DK6UG | 838081 | | OE9GHV | 690104 | | EA8BFK | 648670 | | KD2OM | 589003 | | KA7OEI-1 | 576788 | | K4RCG | 571445 | | KPH | 551690 | | K9AN | 480759 | | DF5FH | 480352 | | DJ9PC | 474211 | +--------+------+ only showing top 10 rows Query Time : 5 .821 sec","title":"Top Ten Reporters"},{"location":"spark/top-ten-reporters/#top-ten-reporters","text":"This is a sample application using Scala that performs the following: Reads the Original CSV into a Spark DataFrame Performas a Query Count on Reporters Ordered Descending Reports Top (10) by spot count","title":"Top Ten Reporters"},{"location":"spark/top-ten-reporters/#file-specs","text":"The specs on the test file are: Test File : wsprspots-2020-02.csv Rows : 47,310,649 spots File Size Decompressed : 3.964 GB If you use a different archive, make sure to you pass the relative location to the script when running.","title":"File Specs"},{"location":"spark/top-ten-reporters/#build-and-run","text":"Run the following commands in order, and check your results. # # All commands are run from a terminal # cd ~/Downloads wget -c http://wsprnet.org/archive/wsprspots-2020-02.csv.gz gzip -dk wsprspots-2020-02.csv.gz # set the path of the downloaded and extracted CSV file csvfile = $PWD /wsprspots-2020-02.csv # clone the repo git clone https://github.com/KI7MT/wspr-analytics.git # change directories and build the assembly cd ./wspr-analytics/scala/TopTenReporters # clean and build sbt clean assembly # Runs the following command spark-submit --master local [ 8 ] target/scala-2.12/TopTenReporter-assembly-1.0.jar $csvfile","title":"Build and Run"},{"location":"spark/top-ten-reporters/#results","text":"You should get results similar to the following: NOTE The time it takes will depend on your system resources (CPU, RAM, etc) Application : TopTenReporter Process File : wsprspots-2020-02.csv Tiimestame : 2020 -12-27 T 02 :36:01.265 Description : Returns the Top Ten Reporters Grouped by Count Process Steps for this application - Creating the Schema - Reading CSV into DataSet - Selecting Reporters - GroupBy and Count Reporters - Sort Reporters Descending - Query Execution +--------+------+ | Reporter | count | +--------+------+ | DK6UG | 838081 | | OE9GHV | 690104 | | EA8BFK | 648670 | | KD2OM | 589003 | | KA7OEI-1 | 576788 | | K4RCG | 571445 | | KPH | 551690 | | K9AN | 480759 | | DF5FH | 480352 | | DJ9PC | 474211 | +--------+------+ only showing top 10 rows Query Time : 5 .821 sec","title":"Results"},{"location":"tools/install-grafana/","text":"Under Development This section will discuss the various ways to install Grapaha.","title":"Install Graphana"},{"location":"tools/install-java/","text":"Under Development This section will discuss the various ways to install Java.","title":"Install Java"},{"location":"tools/install-postgresql/","text":"Under Development This section will discuss the various ways to install PostgreSQL.","title":"Install PostgreSQL"},{"location":"tools/install-python/","text":"Under Development This section will discuss the various ways to install Python.","title":"Install Python"},{"location":"tools/install-r/","text":"At the time of this writing, usage has only been tested on Linux x86-64 and. There is no particular reason why installation and usage could not be done on Windows or Mac OSX if the Requirements are met. It is Highly Recommended to use Anaconda from Continuum Analytics. It's free, supports all major operating systems, is well supported, does not interfere with system level packaging, can be installed on a per user basics and provides everything needed for Comprehensive WSPR Data Analysis . Installing Anaconda \u00b6 Installing Anaconda is very easy. Simply download the shell script and run it in a terminal: Use their Install Instructions paying particular atention to the additional package needs for Qt. Follow the prompts and when asked, I elected to add the the source scripts to my .bashrc but that is entirely up to you. I also used the default installation direcotry of: /home/ $USER /anaconda3 Upgrade Anaconda \u00b6 This is part of 30 Minute Conda getting started page, but for completeness, I'm adding what was needed for my environment. All actions are performed in a terminal, open as required on your system, then: First, Anaconda should be upgraded: conda update conda Next, update the conda-env scripts: conda update conda-env That is all for the basic Anaconda installation and update. You should close, then re-open your terminal to ensure all the paths and updates are working proerpy. Additional Python Modules \u00b6 One package that is not available is a conda package is AppDirs but can be installed with Pip . In the terinal: pip install appdirs clint Installing R \u00b6 The R-Scripting language in not part of the base Anaconda installation, however, installation is fairly easy using conda , the Anaconda package manager. Again, in the terminal, perform the following: conda install -c r r-essentials conda install -c r r-gridextra","title":"Install R"},{"location":"tools/install-r/#installing-anaconda","text":"Installing Anaconda is very easy. Simply download the shell script and run it in a terminal: Use their Install Instructions paying particular atention to the additional package needs for Qt. Follow the prompts and when asked, I elected to add the the source scripts to my .bashrc but that is entirely up to you. I also used the default installation direcotry of: /home/ $USER /anaconda3","title":"Installing Anaconda"},{"location":"tools/install-r/#upgrade-anaconda","text":"This is part of 30 Minute Conda getting started page, but for completeness, I'm adding what was needed for my environment. All actions are performed in a terminal, open as required on your system, then: First, Anaconda should be upgraded: conda update conda Next, update the conda-env scripts: conda update conda-env That is all for the basic Anaconda installation and update. You should close, then re-open your terminal to ensure all the paths and updates are working proerpy.","title":"Upgrade Anaconda"},{"location":"tools/install-r/#additional-python-modules","text":"One package that is not available is a conda package is AppDirs but can be installed with Pip . In the terinal: pip install appdirs clint","title":"Additional Python Modules"},{"location":"tools/install-r/#installing-r","text":"The R-Scripting language in not part of the base Anaconda installation, however, installation is fairly easy using conda , the Anaconda package manager. Again, in the terminal, perform the following: conda install -c r r-essentials conda install -c r r-gridextra","title":"Installing R"},{"location":"tools/install-scala/","text":"Under Development This section will discuss the various ways to install Scala","title":"Install Scala"},{"location":"tools/install-sdkman/","text":"Under Development This section will discuss the various ways to install sdkman.","title":"Install Sdkman"},{"location":"tools/install-spark/","text":"Under Development This section will discuss the various ways to install Spark .","title":"Install Spark"},{"location":"tools/install-vagrant/","text":"Under Development This section covers installing Hashi Corp Vagrant using VirtualBox as the Provider .","title":"Install Vagrant"},{"location":"tools/install-virtualbox/","text":"Under Development This section describes installing VirtualBox and Extension Pack . Version Information \u00b6 Virtual Box - 6.1.16 Extension Pack - 6.1.16 Extension Notes \u00b6 Support for USB 2.0 and USB 3.0 devices, VirtualBox RDP, disk encryption, NVMe and PXE boot for Intel cards. Please install the same version extension pack as your installed version of VirtualBox. -- VirtualBox Org --","title":"Install VirtualBox"},{"location":"tools/install-virtualbox/#version-information","text":"Virtual Box - 6.1.16 Extension Pack - 6.1.16","title":"Version Information"},{"location":"tools/install-virtualbox/#extension-notes","text":"Support for USB 2.0 and USB 3.0 devices, VirtualBox RDP, disk encryption, NVMe and PXE boot for Intel cards. Please install the same version extension pack as your installed version of VirtualBox. -- VirtualBox Org --","title":"Extension Notes"},{"location":"tools/overview/","text":"Under Development This section will be the the installation guide overview page.","title":"Overview"},{"location":"wsprdaemon/database-notes/","text":"The content on the page are derived directly from the work of Gwyn Griffiths, G3ZIL at WSPR Daemon Org . The examples providered are simply re-formated for easier readablity in Markdown Web Page Docs and included in code-blocks. If a Python example does not exist for a particular Pgsql query, a sample script may be provided depending on input parameter complexity and desired ouput. Authors and Credits \u00b6 Principal Author - Gwyn Griffiths, G3ZIL Doc ref - Wspr Daemon Timescale Databases Notes Before working through these example, one should read the reference material in its entirity before using anything from this site. Schmenas and Data Types \u00b6 The database schema and data-types are defined in the Reference Document . Any deviation from the reference should be clearly understood by those running or authoring additional scripts.","title":"Database Notes"},{"location":"wsprdaemon/database-notes/#authors-and-credits","text":"Principal Author - Gwyn Griffiths, G3ZIL Doc ref - Wspr Daemon Timescale Databases Notes Before working through these example, one should read the reference material in its entirity before using anything from this site.","title":"Authors and Credits"},{"location":"wsprdaemon/database-notes/#schmenas-and-data-types","text":"The database schema and data-types are defined in the Reference Document . Any deviation from the reference should be clearly understood by those running or authoring additional scripts.","title":"Schmenas and Data Types"},{"location":"wsprdaemon/overview/","text":"WsprDaemon is a robust application for Linux systems that: Decodes Weak Signal Propagation Reporter (WSPR) signals (Franke/Taylor wsprd* program) Operates with networked KiwiSDRs for multi-channel spot acquisition (at least 8 channels) Includes baseband audio input option Provides a complete, robust and reliable reporting function to the wsprnet.org database Estimates noise using RMS and FFT methods at same time and frequencies as WSPR spots Reports spots and noise to a Timescale database with Grafana visualisation interface Serves data to 3 rd party apps including VK7JJ and WSPR Watch -- WSPR Daemon Org -- Project Resoureces \u00b6 The links on this page are pulled directly from WSPR Daemon Org and placed here for quick reference. Please see the site for additioanl resources. Additional content that should be reviewed: Wspr Daemon Timescale Databases Notes by Gwyn Griffiths, G3ZIL Primary Links \u00b6 WSPR Daemon Groups IO Site Github Project Help guides Presentations Technical Visuialization and Tools \u00b6 Noise Graphs Grafana Server Grafana Help Guide Third Party Integrations \u00b6 VK7JJ WSPR Watch","title":"Overview"},{"location":"wsprdaemon/overview/#project-resoureces","text":"The links on this page are pulled directly from WSPR Daemon Org and placed here for quick reference. Please see the site for additioanl resources. Additional content that should be reviewed: Wspr Daemon Timescale Databases Notes by Gwyn Griffiths, G3ZIL","title":"Project Resoureces"},{"location":"wsprdaemon/overview/#primary-links","text":"WSPR Daemon Groups IO Site Github Project Help guides Presentations Technical","title":"Primary Links"},{"location":"wsprdaemon/overview/#visuialization-and-tools","text":"Noise Graphs Grafana Server Grafana Help Guide","title":"Visuialization and Tools"},{"location":"wsprdaemon/overview/#third-party-integrations","text":"VK7JJ WSPR Watch","title":"Third Party Integrations"}]}